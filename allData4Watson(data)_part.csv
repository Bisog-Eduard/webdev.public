"Postpartum depression prediction through pregnancy data analysis for emotion-aware smart systems updates; Emotion-aware smart systems; Cognitive computing; Information fusion; Mental health solutions; Maternal care quality improvement; SENSOR NETWORKS; CLASSIFICATION; RECOGNITION; ALGORITHMS; INTERNET; ANXIETY; FUSION; RISK. Emotion-aware computing represents an evolution in machine learning enabling systems and devices process to interpret emotional data to recognize human behavior changes. As emotion-aware smart systems evolve, there is an enormous potential for increasing the use of specialized devices that can anticipate life-threatening conditions facilitating an early response model for health complications. At the same time, applications developed for diagnostic and therapy services can support conditions recognition (as depression, for instance). Hence, this paper proposes an improved algorithm for emotion-aware smart systems, capable for predicting the risk of postpartum depression in women suffering from hypertensive disorders during pregnancy through biomedical and sociodemographic data analysis. Results show that ensemble classifiers represent a leading solution concerning predicting psychological disorders related to pregnancy. Merging novel technologies based on IoT, cloud computing, and big data analytics represent a considerable advance in monitoring complex diseases for emotion aware computing, such as postpartum depression.""Verifiable outsourced computation over encrypted data; Outsourced computation; Verifiability; Privacy preservation; Fully homomorphic encryption; Polynomial factorization algorithm; . In recent years, cloud computing has become the most popular and promising service platform. A cloud user can outsource its heavy computation overhead to a cloud service provider (CSP) and let the CSP make the computation instead. In order to guarantee the correctness of the outsourced processing (e.g., machine learning and data mining), a proof should be provided by the CSP in order to make sure that the processing is carried out properly. On the other hand, from the security and privacy points of view, users will always encrypt their sensitive data first before they are outsourced to the CSP rather than sending the raw data directly. However, processing and verifying of encrypted data computation has always been a challenging problem. Homomorphic Encryption (HE) has been proposed to tackle this task on computations over encrypted data and ensure the confidentiality of the data. However, original HE cannot provide an efficient approach to verify the correctness of computation over encrypted data that is processed by CSP. In this paper, we propose a verifiable outsourced computation scheme over encrypted data with the help of fully homomorphic encryption and polynomial factorization algorithm. Our scheme protects user data security in outsourced processing and allows public verification on the computation result processed by CSP with zero knowledge. We then prove the security of our scheme and analyze its performance by comparing it with some latest related works. Performances analysis shows that our scheme reduces the overload of both the cloud users and the verifier.""Secure weighted possibilistic c-means algorithm on cloud for clustering big data; Big data; Possibilistic c-means algorithm; Cloud computing; BGV; . The weighted possibilistic c-means algorithm is an important soft clustering technique for big data analytics with cloud computing. However, the private data will be disclosed when the raw data is directly uploaded to cloud for efficient clustering. In this paper, a secure weighted possibilistic c-means algorithm based on the BGV encryption scheme is proposed for big data clustering on cloud. Specially, BGV is used to encrypt the raw data for the privacy preservation on cloud. Furthermore, the Taylor theorem is used to approximate the functions for calculating the weight value of each object and updating the membership matrix and the cluster centers as the polynomial functions which only include addition and multiplication operations such that the weighed possibilistic c-means algorithm can be securely and correctly performed on the encrypted data in cloud. Finally, the presented scheme is estimated on two big datasets, i.e., eGSAD and sWSN, by comparing with the traditional weighted possibilistic c-means method in terms of effectiveness, efficiency and scalability. The results show that the presented scheme performs more efficiently than the traditional weighted possiblistic c-means algorithm and it achieves a good scalability on cloud for big data clustering.""New publicly verifiable computation for batch matrix multiplication; Publicly verifiable computation; Public delegation; Batch matrix multiplication; Privacy protection; LARGE-SCALE SYSTEMS; CLOUD; SECURE; EFFICIENT; SERVICE; ALGORITHMS; DELEGATION; DATABASE. With the prevalence of cloud computing, the resource constrained clients are trended to outsource their computation-intensive tasks to the cloud server. Although outsourcing computation paradigm brings many benefits for both clients and cloud server, it causes some security challenges. In this paper, we focus on the outsourcing computation of matrix multiplication, and propose a new publicly verifiable computation scheme for batch matrix multiplication. Different from traditional matrix computation outsourcing model, the outsourcing task of our scheme is to compute MXi for group of clients, where X-i is a private matrix chosen by different clients and M is a public matrix given by a data center beforehand. Based on the two techniques of privacy-preserving matrix transformation and matrix digest, our scheme can protect the secrecy of the client's private matrix X-i and dramatically reduce the computation cost in both the key generation and the computing phases. Security analysis shows that the proposed scheme can achieve the desired security properties under the co-computational Diffie-Hellman assumption. We also provide the experimental evaluation that demonstrates the efficiency of our scheme.""Simulating large vCDN networks: A parallel approach; vCDN; Simulation; VM placement; OpenMP; . Virtualization and cloud computing are being used by Communication Service Providers to deploy and utilize virtual Content Distribution Networks (vCDNs) to reduce costs and increase elasticity thereby avoiding performance, quality, reliability and availability limitations that characterize traditional CDNs. As cache placement is based on both the content type and geographic location of a user request, it has a significant impact on service delivery and network congestion. To study the effectiveness of cache placements and hierarchical network architectures composed of sites, a novel parallel simulation framework is proposed utilizing a discretetime approach. Unlike other simulation approaches, the proposed simulation framework can update, in parallel, the state of sites and their resource utilization with respect to incoming requests in a significantly faster manner at hyperscale. It allows for simulations with multiple types of content, different virtual machine distributions, probabilistic caching, and forwarding of requests. In addition, power consumption models allow the estimation of energy consumption of the physical resources that host virtual machines. The results of simulations conducted to assess the performance and applicability of the proposed simulation framework are presented. Results are promising for the potential of this simulation framework in the study of vCDNs and optimization of network infrastructure.""Cloud computing for seizure detection in implanted neural devices; implantable neural stimulators; cloud computing; epilepsy; iEEG; seizure detection; RESPONSIVE CORTICAL STIMULATION; BRAIN-STIMULATION; ELECTRICAL-STIMULATION; EPILEPTIC SEIZURES; LONG-TERM; ONSET; NEUROSTIMULATION; PREDICTION; SECURITY; PRIVACY. Objective. Closed-loop implantable neural stimulators arc an exciting treatment option for patients with medically refractory epilepsy, with a number of new devices in or nearing clinical trials. These devices must accurately detect a variety of seizure types in order to reliably deliver therapeutic stimulation. While effective, broadly-applicable seizure detection algorithms have recently been published, these methods arc too computationally intensive to be directly deployed in an implantable device. We demonstrate a strategy that couples devices to cloud computing resources in order to implement complex seizure detection methods on an implantable device platform. Approach. We use a sensitive gating algorithm capable of running on-board a device to identify potential seizure epochs and transmit these epochs to a cloud-based analysis platform. A precise seizure detection algorithm is then applied to the candidate epochs, leveraging cloud computing resources for accurate seizure event detection. This seizure detection strategy was developed and tested on eleven human implanted device recordings generated using the NeuroVista Seizure Advisory System. Main results. The gating algorithm achieved high-sensitivity detection using a small feature set as input to a linear classifier, compatible with the computational capability of next-generation implantable devices. The cloud-based precision algorithm successfully identified all seizures transmitted by the gating algorithm while significantly reducing the false positive rate. Across all subjects, this joint approach detected 99% of seizures with a false positive rate of 0.03 h(-1). Significance. We present a novel framework for implementing computationally intensive algorithms on human data recorded from an implanted device. By using telemetry to intelligently access cloud-based computational resources, the next generation of neuro-implantable devices will leverage sophisticated algorithms with potential to greatly improve device performance and patient outcomes.""Practical public key encryption with selective opening security for receivers; Data security; Selective opening attacks; Chosen-ciphertext attacks; Receivers; PROTECTION; EFFICIENT. Data security and privacy protection is one of the highest concerns in cloud computing. Selective opening security (SOA security) is a security notion focusing oil a multi-user setting on the Internet. In recent years, many public-key encryption (PKE) constcutions have been proposed that meet SOA security. Most of them focus on the sender corruption setting. However, less attention has been paid on the receiver corruption setting. Inspired by the work of Heuer et al. (PKC 2015), which showed a practical SOA secure PKE scheme (in the sender corruption setting) from a key encapsulation mechanism (KEM) and a message authentication code (MAC), we propose a practical generic PKE construction achieving simulation-based SOA security under chosen-ciphertext attacks for recievers (RSIM-SO-CCA security). Our construction is also based on a KEM and a MAC. We show that if the underlying KEM is one-way secure in the presence of a plaintext-checking oracle (OW-PCA) and the underlying MAC is strong unforgeable under one-time chosen message attacks (sUF-OT-CMA), then our generic construction is RSIM-SO-CCA secure in the random oracle model.""Crosstalk-aware routing, spectrum, mode and time assignment using FMF with partial MIMO equalization in flexible grid datacenter networks; Datacenter network (DCN); Elastic optical network (EON); Spatial division multiplexing (SDM); Few mode fiber (FMF); Routing, spectrum, mode and time assignment (RSMTA); WAVELENGTH; ALLOCATION. Recently, lots of datacenters (DCs) have been built to enhance reliability and expand processing ability for computing service in cloud computing. Furthermore, spatial division multiplexing (SDM) technology and elastic optical network (EON) are also considered as the candidate solutions to solve the requirements of huge bandwidth and flexible connections. However, it is much more difficult to use few mode fiber (FMF) in DCs-based EON due to the existing modal crosstalk among different modes. Faced with the challenges, we propose a routing, spectrum, mode and time assignment algorithm to decrease the blocking probability and increase the spectrum utilization. To deal with the problem more efficiently, we propose a method called flexible sliding three-dimensional block (3D block) which represents the dimensions of spectrum, space and time in our algorithm. 3D block slides in the resource space to find out proper resources. The simulations are conducted on datacenter networks (DCNs) and the results show that lower blocking probability and higher spectrum utilization are achieved by efficiently utilizing the spectrum, space and time domains.""Guarded dual authentication based DRM with resurgence dynamic encryption techniques; Twofold authentication protocol; Efficient Elliptic Curve Public Key Encryption (EECPKE); integrity checking protocol; Covariance matrix Adaptation Evolution Strategies (CMA-ES); PROXY RE-ENCRYPTION; DATA-SECURITY; CLOUD; SCHEME; PROTOCOL; PRIVACY; STORAGE; ISSUES. Cloud computing is the emergent technology that face one of the significant issues time with data security while outsourcing the data onto the cloud in recent. Some cryptographic techniques have been used for protection in form of identity, attributes and prediction algorithms nonetheless these algorithms lack their performance and becomes are very prone to attackers when an unauthorized user reunited the system with dissimilar way for privileges to the similar data files. The essential need of this data security solved by some enhanced cryptographic techniques in DRM utilizing a secure privacy preserving data sharing with encryption techniques of Dynamic Unidirectional Proxy Re-Encryption. This technique is based on Cipher text Policy Attribute by providing the privacy, integrity and security of the data while retrieving.""Do Electronic Word-of-Mouth and Elaboration Likelihood Model Influence Hotel Booking?; Artificial neural networks; booking intention; demographic; elaboration-likelihood model; eWoM; USER-GENERATED CONTENT; CLOUD-BASED VLE; MODERATING ROLE; BEHAVIORAL INTENTION; MOBILE ENTERTAINMENT; SOURCE CREDIBILITY; NETWORK ANALYSIS; ONLINE REVIEWS; INITIAL TRUST; EWOM. The emergences of Web 2.0 and cloud computing have contributed greatly to the prevalence of electronic Word-of-Mouth (eWoM). Unlike most of the existing studies which have used linear models, nonlinear relationships were discovered in hotel booking intention. So far, the effects of elaboration-likelihood model (ELM) and demographics have been largely overlooked, though studies have shown that ELM may explain consumers' perception, behavior, and IS acceptance. Data were gathered from 497 patrons of 10 hotels in Golden Triangle, Kuala Lumpur, Malaysia. Using artificial neural network (ANN), we found that user involvement, positive eWoM, user expertise, perceived credibility, education, negative eWoM, and income are among the important predictors explaining 81% of variance in booking intention. The theoretical implications may further advance eWoM and ELM literatures, while the managerial implications may provide novel understandings to hotel operators, advertisers, and relevant hospitality policy makers in formulating effective decision making.""Data Lakes, Clouds, and Commons: A Review of Platforms for Analyzing and Sharing Genomic Data; ; CANCER; VISION; BROWSER; GALAXY. Data commons collate data with cloud computing infrastructure and commonly used software services, tools, and applications to create biomedical resources for the large-scale management, analysis, harmonization, and sharing of biomedical data. Over the past few years, data commons have been used to analyze, harmonize, and share large-scale genomics datasets. Data ecosystems can be built by interoperating multiple data commons. It can be quite labor intensive to curate, import, and analyze the data in a data commons. Data lakes provide an alternative to data commons and simply provide access to data, with the data curation and analysis deferred until later and delegated to those that access the data. We review software platforms for managing, analyzing, and sharing genomic data, with an emphasis on data commons, but also cover data ecosystems and data lakes.""The design of an IoT-GIS platform for performing automated analytical tasks; Internet of things; Automated analytical tasks; Mobility context; Smart transit; INTERNET; THINGS; ARCHITECTURE; INTEGRATION. Society has a very ambitious vision of building smart interconnected cities through the Internet of Things (Ion. Billions of data streams will be generated by devices using different networking infrastructures of smart cities, enabling the automation of how the data that are being collected can be analysed for. However, significant scientific and technological challenges need to be overcome before IoT-GIS platforms can be widely used. This paper is a first step towards designing an IoT-GIS platform for performing automated analytical tasks that are able to retrieve, integrate and contextualize data streams with the purpose of adding value to the provision of transit services. Three automated tasks are used to describe our platform: (1) data ingestion for retrieving data streams; (2) data cleaning for handling missing and redundant data streams; and (3) data contextualization for representing the mobility context of transit driving behaviour. The Codiac Transit System of the Greater Moncton area, NB, Canada was used for building a mobility context and evaluating the cloud architecture that was used to implement our IoT-GIS platform. From the experimental results, the need for cloud computing for achieving scalability and high performance of our IoT-GIS platform is validated. Suggestions for the operational management of routes to improve service quality are proposed based on the analytical outcomes.""Toward Ultra-Low-Power Remote Health Monitoring: An Optimal and Adaptive Compressed Sensing Framework for Activity Recognition; Compressed sensing; activity recognition; feature selection; energy efficiency; ultra-low power; optimization; adaptive; WIRELESS SENSOR NETWORKS. Activity recognition, as an important component of behavioral monitoring and intervention, has attracted enormous attention, especially in Mobile Cloud Computing (MCC) and Remote Health Monitoring (RHM) paradigms. While recently resource constrained wearable devices have been gaining popularity, their battery life is limited and constrained by the frequent wireless transmission of data to more computationally powerful back-ends. This paper proposes an ultra-low power activity recognition system using a novel adaptive compressed sensing technique that aims to minimize transmission costs. Coarse-grained on-body sensor localization and unsupervised clustering modules are devised to autonomously reconfigure the compressed sensing module for further power saving. We perform a thorough heuristic optimization using Grammatical Evolution (GE) to ensure minimal computation overhead of the proposed methodology. Our evaluation on a real-world dataset and a low power wearable sensing node demonstrates that our approach can reduce the energy consumption of the wireless data transmission up to 81.2 and 61.5 percent, with up to 60.6 and 35.0 percent overall power savings in comparison with baseline and a naive state-of-the-art approaches, respectively. These solutions lead to an average activity recognition accuracy of 89.0 percent-only 4.8 percent less than the baseline accuracy-while having a negligible energy overhead of on-node computation.""A mixed-method empirical study of Function-as-a-Service software development in industrial practice; Cloud computing; Serverless; Function-as-a-Service; Empirical research; MICROSERVICES. Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of ""serverless"" computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the ""glue"" that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers.""Harnessing the power of big data analytics in the cloud to support learning analytics in mobile learning environment; Mobile learning (m-learning); Learning analytics; Big data analytics; Cloud computing; Map reduce technique; Technology acceptance model (TAM); TECHNOLOGY; ACCEPTANCE. Technology enhanced learning (TEL) such as online learning environment with adaptive technologies has gained growing interest in recent past in the field of teaching and learning. In this context, mobile learning has got much momentum and is exemplified by diverse characteristics associated with the technologies and devices used, the enormous size of data generated throughout a learning session, and the interactions among the learners that occur outside the classroom. Consequently, sophisticated data analysis techniques are required to handle the intricacy of mobile learning and analyze the vast amount of datasets to enhance the learning experiences of mobile learners. This has led to the adoption of big data analytics for efficient processing of big learning data to add value to the mobile learning environments. Yet limited processing capability of the mobile devices is another key challenge faced by such big data analytics in mobile learning environments. To overcome this limitation, certain heavy computational parts could be offloaded to the cloud which can provide enough computation and storage resources. To this end, this paper presents a cloud based mobile learning framework that utilizes big data analytics technique to extract values from huge volume of mobile learners' data. Finally, we investigate learners' readiness and driving factors of mobile learning adoption in higher education institutions. In particular, we propose a hypothesized model for mobile learning adoption built on a locally extended technology acceptance model (TAM).""A dynamic information platform for underground coal mine safety based on internet of things; Internet of things; Big data; Underground coal mine; Mining safety; BIG DATA; IOT; DESIGN; ACCIDENTS; FUTURE; CITIES; SYSTEM. Mechanical and intelligent mining technology reduces labor intensity, accidents and casualties while improving mining efficiency in underground coal mines. Such advanced mining methods are gradually realized in China with increasing application of an Internet of Things (IoT) technique. In this study, a dynamic information platform is established for underground coal mine based on IoT technology. The platform aims to satisfy various objectives of potential users through combining four subsystems, including data acquisition, transmission, analyzation and application systems. To achieve such functions, six functional layers are composed of the platform, including the supporting layer, perception layer, transmission layer, service layer, data extraction layer and application layer. The platform can monitor and record working conditions data of the coal mine production systems as well as location information of underground equipment and miners. Based on cloud computing techniques, big data related to underground coal mining can be analyzed quickly, and critical data associated with user requirements can be extracted precisely. The platform provides a 3D virtual mine system, safety diagnosis system, safety inspection system and emergency rescue system for coal mines. With the established platform, the danger-spotting and subsequent decision-making capabilities of users can be greatly enhanced to ensure underground mining safety.""Efficient 3D probabilistic stability analysis of rock tunnels using a Lattice Model and cloud computing; Lattice; Probabilistic; Rock tunnel; Cloud computing; NUMERICAL-SIMULATION; STOCHASTIC-ANALYSIS; FRACTURE; STRENGTH; BLOCK; NETWORKS; GEOMETRY; BEHAVIOR; PARTICLE. In this paper, the Classic Lattice Spring Model is developed in order to be applied to probabilistic stability analyses of rock tunnels in low in situ stress environments, where the falling of blocks is the main failure mechanism. For this objective, the method is combined with the Synthetic Rock Mass technique and Barton-Bandis joint constitutive model, giving it a series of advantages: simulations with deformable blocks; possible formation of tension cracks; and an appropriate constitutive model for the mechanical behavior of joints. The use of cloud computing technology is proposed for efficient probabilistic analyses using the Monte Carlo simulation. A case study is presented based on an unsupported section of a shallow rock tunnel excavated in the state of Espirito Santo, Brazil. A 3D probabilistic stability analysis of the tunnel is performed with the proposed methodology. From the results, a positional probability map is elaborated, which indicates the likelihood of block failure around the excavation cross-section. The unstable zones indicated by the map are then compared to the failed region of the real tunnel so as to demonstrate the competence of the methodology.""Resource allocation and routing in parallel multi-server queues with abandonments for cloud profit maximization; Parallel multi-server queues; Abandonments; Firm deadlines; Resource allocation; Routing; Markov decision process; Cloud computing; Bernoulli splitting; Index policies; LOAD DISTRIBUTION; OPTIMAL REPAIR; REAL-TIME; CUSTOMERS; OPTIMALITY; ADMISSION; SYSTEMS; SERVICE. This paper considers a Markov decision model for profit maximization of a cloud computing service provider catering to customers submitting jobs with firm real-time random deadlines. Customers are charged on a per-job basis, receiving a full refund if deadlines are missed. The service provider leases computing resources from an infrastructure provider in a two-tier scheme: long-term leasing of basic infrastructure, consisting of heterogeneous parallel service nodes, each modeled as a multi-server queue, and short-term leasing of external servers. Given the intractability of computing an optimal dynamic resource allocation and job routing policy, maximizing the long-run average profit rate, the paper addresses the design, implementation and testing of low-complexity heuristics. The policies considered are a static policy given by an optimal Bernoulli splitting, and three dynamic index policies based on different index definitions: individually optimal (IO), policy improvement (PI) and restless bandit (RB) indices. The paper shows how to implement efficiently each such policy, and presents a comprehensive empirical comparison, drawing qualitative insights on their strengths and weaknesses, and benchmarking their performance in an extensive study.""On semantic detection of cloud API (anti)patterns; Cloud computing; REST; OCCI; Pattern; Anti-pattern; Analysis; Specification; Detection; Ontology; . Context: Open standards are urgently needed for enabling software interoperability in Cloud Computing. Open Cloud Computing Interface (OCCI) provides a set of best design principles to create interoperable REST management APIs. Although OCCI is the only standard addressing the management of any kind of cloud resources, it does not support a range of best principles related to REST design. This often worsens REST API quality by decreasing their understandability and reusability. Objective: We aim at assisting cloud developers to enhance their REST management APIs by providing a compliance evaluation of OCCI and REST best principles and a recommendation support to comply with these principles. Method: First, we leverage patterns and anti-patterns to drive respectively the good and poor practices of OCCI and REST best principles. Then, we propose a semantic-based approach for defining and detecting REST and OCCI (anti)pattems and providing a set of correction recommendations to comply with both REST and OCCI best principles. We validated this approach by applying it on cloud REST APIs and evaluating its accuracy, usefulness and extensibility. Results: We found that our approach accurately detects OCCI and REST(anti)patterns and provides useful recommendations. According to the compliance results, we reveal that there is no widespread adoption of OCCI principles in existing APIs. In contrast, these APIs have reached an acceptable level of maturity regarding REST principles. Conclusion: Our approach provides an effective and extensible technique for defining and detecting OCCI and REST (anti)patterns in Cloud REST APIs. Cloud software developers can benefit from our approach and defined principles to accurately evaluate their APIs from OCCI and REST perspectives. This contributes in designing interoperable, understandable, and reusable Cloud management APIs. Thank to the compliance analysis and the recommendation support, we also contribute to improving these APIs, which make them more straightforward.""Modelling the utilization of cloud health information systems in the Iraqi public healthcare sector; Health information system; Health informatics; e-Health; Cloud computing; Model; Iraq; TECHNOLOGY ACCEPTANCE MODEL; CRITICAL SUCCESS FACTORS; PERCEIVED CONTROL; USER ACCEPTANCE; PRIVACY; DETERMINANTS; EXTENSION; CONTINUANCE; ADOPTION; INTENTIONS. The Iraqi healthcare sector has been suffering from health records management issues from the perspectives of low information technology integrity and data complexity. As a solution, cloud computing services can offer an alternative, low-cost, and reliable way to store, manage, and retrieve health-related data, as well as monitoring patients' health conditions anywhere and anytime using any device with any platform. The migration to cloud services is not yet widespread in Iraqi health facilities due to various challenges, including security and privacy, legal policies, and implementation. For instance, there has been no research shedding light on the utilization of cloud computing services in Iraqi hospitals' health information systems. This study proposed a model by defining the critical success factors influencing physicians' confirmation and behavioral control toward utilizing cloud health information systems in Iraqi hospitals. The model's variables were statistically investigated by utilizing an online questionnaire. Data were collected from a probability sample of 259 physicians working in four Iraqi high-IT hospitals. The collected ordinal data were analyzed using the PLS-SEM approach as a nonparametric second generation multivariate analysis. The results showed that the effects of system compatibility, system complexity, security, and privacy on physicians' confirmation and behavioral control were statistically significant. Both confirmation and behavioral control had a positive effect on physicians' utilization of the technology in the Iraqi hospitals. It is believed that such finding may help to aid the current understanding of cloud health systems in managing health data as well as providing the necessary recommendations for policy makers to direct healthcare professionals to continuously consider the use of modern information and communications technology in the workplace.""Exploring cloud-based Web Processing Service: A case study on the implementation of CMAQ as a Service; CMAQ; WPS; Cloud computing; Earth science model; Web service; OGC; MODELS; ECOSYSTEM; QUALITY; SYSTEM. As an important tool for air quality simulation, the Community Multiscale Air Quality (CMAQ) model is widely used in the environmental modeling community. However, setting up and running the CMAQ model could be challenging for many scientists, especially when they have limited computing resources and little experience in handling large-scale input data. In this study, we explore the cloud-based Web Processing Service (WPS) and present the Cloud WPS framework to support implementing Earth science model as WPS. Specifically, to make CMAQ easier to use for scientists through the latest standard-based Web service technology, CMAQ-WS, a prototype of CMAQ as a Service, is developed and tested. The result of the experiment shows the framework significantly improves not only the performance of but also ease of use of the CMAQ model thus providing great benefits to the environmental modeling community. Meanwhile, the proposed framework provides a general solution to integrate Earth science model, WPS, and cloud infrastructure, which can greatly reduce the workload of Earth scientists.""BMMI-tree: A Peer-to-Peer m-ary tree using 1-m node splitting for an efficient multidimensional complex query search; Distributed computing; Peer-to-peer networks; Multidimensional indexing; Node splitting; Multiway tree structures; Complex query search; DISCOVERY; SERVICE; OVERLAY. Peer-to-Peer (P2P) applications such as content distribution and sharing (like file, audio, video), multiuser communication (games, desktop sharing, e-learning) have emerged as a new paradigm over a last decade. However, scalability requirements remain a major concern and hence, the distribution and effective search of multidimensional data have become major challenges for P2P computing. Most of the existing P2P overlays either do not give support to Multidimensional Indexing (MI) or the frameworks are less efficient for complex query search or they are limited up to binary trees only, with the search complexity O(log(2)N). However, traditional MI based on m-ary tree is strengthened for the complex query search (bound to log(m)N) using higher fanout, m > 2. Based on these observations, we propose BMMI-tree (Balanced Multiway Multidimensional Indexing-tree) that uses an m-ary P2P tree overlay network and also provides the support of MI tree indexing methods such as R-tree or SS-tree in this paper. The paper also analyzes the complex query search algorithms performed in O(log(m)N) steps with the experimental results. In addition, the construction of the P2P tree network requires to split some existing node and its data objects into m new child nodes (during node join) and vice versa (during node leave). To the best of our knowledge, none of the existing node splitting algorithms for multiway multidimensional trees offer 1-m node splitting. Hence, in this paper, we also propose two different approaches to split the MI tree node into m number of nodes (m-ary split) to be used effectively to create a dynamic tree overlay. Lastly, we present how the BMMI-tree can be applied for service provisioning in cloud computing in a decentralized and distributed manner.""QaMeC: A QoS-driven IoVs application optimizing deployment scheme in multimedia edge clouds; IoVs (Internet of Vehicles); IoT(Internet of Things); Optimizing deployment; Cloud computing; Edge computing; Multi-clouds; QoS; CDN (Content Delivery Network, Content Distribution Network); . Deploying applications to a centralized cloud for service delivery is infeasible because of the excessive latency and bandwidth limitation of the Internet, such as transporting all IoVs data to big data processing service in a centralized cloud. Therefore, multi-clouds, especially multiple edge clouds is a rising trend for cloud service provision. However, heterogeneity of the cloud service, complex deployment requirements, and large problem space of multi-clouds deployment make how to deploy applications in the multi-clouds environment be a difficult and error-prone decision-making process. Due to these difficulties, current SIA-based solution lacks a unified model to represent functional and non-functional requirements of users. In this background, we propose a QoS-driven IoVs application optimizing deployment scheme in multimedia edge clouds (QaMeC). Our scheme builds a unified QoS model to shield off the inconsistency of QoS calculation. Moreover, we use NSGA-II algorithm to solve the multi-clouds application deployment problem. The implementation and experiments show that our QaMeC scheme can provide optimal and efficient service deployment solutions for a variety of applications with different QoS requirements in CDN multimedia edge clouds environment.""ITEMa: A methodological approach for cognitive edge computing IoT ecosystems; IoT-based ecosystems; Cognitive systems; Edge and cloud computing; Smart office; Activity recognition; SMART ENVIRONMENTS; INTERNET; SYSTEMS; THINGS; PLATFORM; DESIGN. The ever-increasing spread of Internet of Things (IoT)-based technologies paired with the diffusion of the edge-based computing boosts the development of pervasive cyber ecosystems having the goal of improving the life quality of people and assisting them in daily activities. In this context, cognitive behaviors are purposely required to make such ecosystems able to adapt to people needs and to envisage their behaviors. Despite the growing interest in cognitive ecosystems, still there is a lack of methodological approaches devoted to supporting the design and implementation of such complex systems. This paper proposes ITEMa, an Iot-based smarT Ecosystem Modeling Approach based on a three-layered architecture offering some well-suited abstractions tailored to the development of IoT-based ecosystems which exhibit cognitive behaviors and are able to exploit computational resources located either at the edge of the network or in the Cloud. The effectiveness of the approach is demonstrated through a case study concerning the development of a Smart Office devoted to forecast some usual office activities and to properly adapt the office environmental conditions to them.""Speed up genetic algorithms in the cloud using software containers; Genetic algorithms; Parallel genetic algorithms; Cloud computing; Software containers; Software engineering; . Scalability issues might prevent Genetic Algorithms from being applied to real world problems. Exploiting parallelisation in the cloud might be an affordable approach to getting time efficient solutions that benefit of the appealing features of scalability, resource discovery, reliability, fault-tolerance and costeffectiveness. Nevertheless, parallel computation is very prone to cause considerable overhead for communication. Also, making Genetic Algorithms distributed in an on-demand fashion is not trivial. Aiming at keeping under control the communication cost and, at the same time, supporting developers in the construction and deployment of parallel Genetic Algorithms, we designed and implemented a novel approach to distribute Genetic Algorithms in the form of a cloud-based application. It is based on the master/slave model, exploiting software containers, their cloud orchestration and message queues. We also devised a conceptual workflow covering each cloud Genetic Algorithms distribution phase, from resources allocation to actual deployment and execution, in an engineered fashion. Then, the application performance has been evaluated using a benchmark problem. According to the performance and setup times results, it emerged that the cloud can be considered a compelling way of scaling Genetic Algorithms and an excellent alternative to other technologies strictly related to the physically owned hardware.""Zeus: A resource allocation algorithm for the cloud of sensors; Cloud of sensors; Edge computing; Resource allocation; Mixed integer non-linear programming; Data provisioning; Data freshness; WIRELESS SENSOR; ENERGY-EFFICIENT; INTERNET; PERFORMANCE; SELECTION; SYSTEM. The cloud of sensors (CoS) paradigm brings together cloud computing, edge computing and wireless sensor and actuator networks (WSAN) in the form of a three-tier CoS architecture. By employing CoS virtualization, a set of virtual nodes (VNs) is made available to applications, decoupling them from the CoS infrastructure. Assigning VNs to application requests, in order to timely and efficiently meet application requirements, gives rise to the challenge of resource allocation in CoS. In this work, we formulate the problem of resource allocation in CoS and propose Zeus, a hybrid (partly-decentralized) algorithm for solving it. Zeus has two key features. First, it is able to identify requests that are common for multiple applications and perform only once their required tasks, sharing the results among the applications for saving resources. Second, its hybrid approach leveraging edge computing, makes Zeus scalable in terms of the number of VNs and applications supported, and suitable for meeting delay-sensitive applications.""Cross-layer multiuser session control for optimized communications on SDN-based cloud platforms; Cloud computing; Internet of things; Software defined networking; Cross-layer control plane; VIDEO; QUALITY; DELIVERY; SERVICE; ARCHITECTURE; EXPERIENCE; INTERNET; ISSUES; THINGS. The plethora of emerging applications and their increasing popularity has led to the cloudification of the Internet of Things (IoT). The IoT-cloudification scales up data centers to a high computational resource capacity, to suit the requirements of a large number of users wanting to access multiple services offered by revolutionary IoT applications which, in turn, consume huge amounts of data produced by billions of things. In such systems, IoT-tailored cloud services (e.g., software platforms and middleware) are responsible for running latency-critical IoT applications in different areas (e.g., healthcare, transport systems, safety, Smart Cities, and many others). These applications expect to produce high-density multimedia flows that must be handled appropriately by the underlying communication infrastructure to obtain transport services of the highest standard. In this work, we propose a novel control plane optimization, which leverages the support of the Software-Defined Networking (SDN) substrate so that it can advance beyond today's IP-based communication systems, which reveals weaknesses (e.g. a limited performance and scalability) in delivering multimedia content through per-flow channels. An attempt is made to overcome these limitations by proposing CLASSICO, a Cross-LAyer Sdn Session COntrol approach that exploits the SDN substrate to offload the flow streaming network-computing task from the IoT cloud platform to selected SDN branching nodes, and hence ensures high timeliness and scalability for the IoT-cloud system as a whole. To achieve this. CLASSICO dynamically builds application-layer sessions, which comprise multiple participant members sharing same interested content, and connects them into optimal multiuser tree structures, whereby only selected branching nodes apply SDN replication function for quality-enhanced transport service. We applied CLASSICO to a multimedia use case, and the results show that it outperforms multicast-capable IoT systems in terms of Quality of Service (QoS) and Quality of Experience (QoE) video assessment metrics.""SOGC: Implementing surrogate object garbage collector management for a Mobile Cloud Environment; big data management; garbage collection; mark-compact; memory management; Mobile Cloud Computing; quality in service; semi-space algorithm; . The aim of an ideal distributed Mobile Cloud environment, the surrogate object, is an agent for a particular Mobile Host in the wired or wireless network with specific data structures and methods. The Surrogate object is a software entity that is hosted on mobile support station and acts on behalf of a mobile host during the disconnection operation. The mobile hosts are registered with their own unique identifier in their cloud environment with their mobile support system. If the registered Mobile host is migrated from one cloud to another, the surrogate object handovers the data or resource management to another cloud with a new object with the same entities of old surrogate object, which leads inefficient. The granularity of object migration the old object is useless after the old object switched over into the new object. The collection of unused surrogate object is a big challenge. This problem will be achieved through the garbage collector technique in Mobile Cloud computing. In this research, we propose the Surrogate Object Garbage Collector (SOGC) model with new middleware consists in detecting the unused surrogate object by Mark-Compact Garbage Collection technique to recycle their object resources. SOGC periodically recycles the idle object and automatically reuses it; whenever it is needed to manage cloud environment, it results in quality-of-execution, low latency, and cost optimization in data management.""Tc-PEDCKS: Towards time controlled public key encryption with delegatable conjunctive keyword search for Internet of Things; Internet of Things; Keywords search; Time control; Keywords guessing attacks; PROXY RE-ENCRYPTION; GUESSING ATTACKS; SECURE; DATABASE; SCHEME; SUBSET. With the widespread adoption of Internet of Things (IoT) and cloud computing, more and more individuals and organizations are outsourcing their IoT data in the cloud server. Under these circumstances, how to protect the privacy and security of the outsourced data becomes a key challenge. Public key encryption with keyword search (PEKS) is a promising technology to solve the problem. In this paper, we present a new kind of PEKS named time controlled public key encryption with delegated conjunctive keyword search (tc-PEDCKS) for IoT deployment. A basic tc-PEDCKS is firstly presented which supports conjunctive keyword search and enables a data user to delegate access right over the data to others in a limited time period. To resist against the keyword guessing attacks, a more secure tc-PEDCKS with designated server construction is also presented. The two constructions are proved secure in the random oracle model. The performance evaluation and experimental results demonstrate that our constructions are practical and feasible for real world IoT scenarios.""A holistic review of Network Anomaly Detection Systems: A comprehensive survey; Intrusion Detection System (IDS); Network Anomaly Detection Systems (NADS); Data pre-processing; Decision Engine (DE); INTRUSION DETECTION SYSTEM; BIG DATA ANALYTICS; CLASSIFICATION; ENSEMBLE; FRAMEWORK; TAXONOMY; ATTACKS; DESIGN. Network Anomaly Detection Systems (NADSs) are gaining a more important role in most network defense systems for detecting and preventing potential threats. The paper discusses various aspects of anomaly-based Network Intrusion Detection Systems (NIDSs). The paper explains cyber kill chain models and cyber-attacks that compromise network systems, Moreover, the paper describes various Decision Engine (DE) approaches, including new ensemble learning and deep learning approaches. The paper also provides more details about benchmark datasets for training and validating DE approaches. Most of NADSs' applications, such as Data Centers, Internet of Things (IoT), as well as Fog and Cloud Computing, are also discussed. Finally, we present several experimental explanations which we follow by revealing various promising research directions.""Internet of Things (IoT), mobile cloud, cloudlet, mobile IoT, IoT cloud, fog, mobile edge, and edge emerging computing paradigms: Disambiguation and research directions; Cloudlet computing; Crowdsensing; Crowdsourcing; Edge computing; Fog computing; Internet of things; Mobile cloud; Mobile cloud computing; Mobile edge computing; Opportunistic sensing; Participatory sensing; Semantic web of things; Web of things; Wisdom web of things; WISDOM WEB; SEMANTIC WEB; AWARE ARCHITECTURE; ENERGY-EFFICIENCY; USER INTERFACES; DATA ANALYTICS; MICRO-CLOUDS; SOCIAL WEB; TRADE-OFF; BIG DATA. Currently, we are experiencing a technological shift, which is expected to change the way we program and interact with the world. Cloud computing and mobile computing are two prominent research areas that have already had such an impact. The Internet of Things (IoT), which is concerned with building a network of Internet-enabled devices to promote a smart environment, is another promising area of research. Numerous emerging computing paradigms related to those areas of research and/or their intersections have come into play. These include Mobile Cloud Computing (MCC), cloudlet computing, mobile clouds, mobile IoT computing, IoT cloud computing, fog computing, Mobile Edge Computing (MEC), edge computing, the Web of Things (WoT), the Semantic WoT (SWoT), the Wisdom WoT (W2T), opportunistic sensing, participatory sensing, mobile crowdsensing, and mobile crowdsourcing. Unfortunately, those paradigms suffer from the lack of standard definitions, and so we frequently encounter a single term referring to various paradigms or several terms referring to a single paradigm. Accordingly, this paper attempts to disambiguate those paradigms and explain how and where they fit in the above three areas of research and/or their intersections before it becomes a serious problem. They are tracked back to their inception as much as possible. This is in addition to discussing research directions in each area. The paper also introduces technologies related to the IoT such as ubiquitous and pervasive computing, the Internet of Nano Things (IoNT), and the Internet of Underwater Things (IoUT).""How to cool hot-humid (Asian) cities with urban trees? An optimal landscape size perspective; Urban tree; Urban cooling island effect; Threshold value of efficiency; Landscape indexes; Urban planning; Hot-humid city; LAND-SURFACE TEMPERATURE; MITIGATION TECHNOLOGIES; GREEN SPACES; HEAT ISLANDS; CLIMATE; RETRIEVAL; PATTERNS; IMPACTS; SHADE; PARKS. Urban areas typically experience higher temperatures compared to surrounding rural areas that is known as the urban heat island effect (UHI). Urban greenery is capable of mitigating the UHI by creating microclimates that are lower in temperature than their surroundings, which are known as urban cooling islands (UCI). Previous studies have proved the effectiveness of UCI from different perspectives. However, a specific optimal level of landscape patch size at a regional scale that can be implemented by urban planners has not been identified. In this study, we estimated the optimal patch size in seven selected hot-humid Asian cities with the help of Google Cloud Computing, Python Programming, as well as spatial and statistical analysis. A two-tier (two optimal patch sizes) distribution of the threshold value of efficiency (TVoE) of urban trees in this region was found. Eight landscape-level indexes were used to explore the variance of TVoE. The percentage of landscape (PLAND), edge density (ED), mean landscape shape index (Shape_MN), mean fractal dimension (FRAC_MN), largest patch index (LPI), and mean Euclidian nearest-neighbor distance (ENN_MN) were found to have no significant correlation with TVoE. While the average normalized difference vegetation index (NDVI_MN) and average background temperature (BGT_MN) were found to be highly associated with the variance in TVoE. Further, a concept model that can simulate the effects of NDVI_MN and BGT_MN was also proposed. These findings extend the understanding of the UCI effect of urban trees as well as providing a basis for scientific climate adaption planning in this region.""GiPlot: An interactive cloud-based tool for visualizing and interpreting large spectral data sets; Plotting large spectral data sets; On-line quick data visualization; Google Apps Script; FLUORESCENCE; WATER. Latest advances in technology and the growing amount of experimental and business data have increased the number of users accessing on-line tools dedicated to quickly visualize and analyse large data sets. This paper describes the development and functionality of a new interactive cloud computing based plotting tool (GiPlot - Google-based Interactive Plot) easy-to-use for universal data. It has interactive features that facilitate data share and interpretation, and selection of specific data suitable for further uses and detailed studies. It also allows quick and step-by-step visualizations of the impact of various experimental conditions on spectral data sets. For a detailed illustration of the features of this interactive plotting tool, we have used mainly spectral data for a given solute dissolved in mixed solvents and for changes in the absorption and/or fluorescence properties of a solute solution in the presence of different chemical stimuli. The most important features and functionalities of this new tool have also been summarized and suggestively highlighted through a short collection of video tutorials containing many examples, developed by the authors of this paper as a support for both the tool and this paper.""Miniature holographic projector with cloud computing capability; ; SPATIAL LIGHT-MODULATOR; PHASE; IMAGE; NOISE; RESOLUTION; ALGORITHM; DISPLAY. A fully functional miniaturized projection head below 5 cm(3) is presented, using computer-generated holograms dynamically displayed on a liquid-crystal spatial light modulator. Spatial division of the modulator is used for color projection without color breakup, and specially designed, anti-reflection coated prisms ensure simple light paths with small losses. Real-time calculations are performed on a remote server with on-the-fly compression of holographic fringes. Cloud computing allows 1 W of local electrical power usage and apparent image brightness equivalent to 15-500 lm/W efficiency, depending on the displayed content. The properties of the projector allow future applications in handheld displays.""An improved attribute-based encryption technique towards the data security in cloud computing; attribute-based encryption; cloud computing; cloud service provider; data owner; distributed hash table network; identity-based timed-release encryption; PEER-TO-PEER; ACCESS-CONTROL; PROTOCOL; OVERLAY. With the rapid development of the Internet, resource and knowledge sharing are two major problems experienced due to the presence of many hackers and malicious users. In this paper, an efficient and secure access control model has been proposed for the cloud computing environment for resource and knowledge sharing by using attribute-based encryption (ABE), distributed hash table (DHT) network, and identity-based timed-release encryption (IDTRE). Here, at first, data or resources are encrypted by using the attributes of users, and encrypted data are divided into the encapsulated ciphertext and extracted ciphertext. Then, IDTRE algorithm has been used to encrypt the decryption key and combined the ciphertext of the key with the extracted ciphertext for creating the ciphertext shares. At last, the ciphertext shares are distributed into the DHT network, and encapsulated ciphertext are stored on the cloud servers. Both the performance and security analysis show the proficiency of the proposed scheme over the existing schemes in a cloud environment.""Big data for cyber physical systems in industry 4.0: a survey; Industry 4.0; IoT; cloud computing; cyber-physical systems; big data; data science; industrial information integration engineering; CLUSTERING METHOD; ALGORITHM; ASSOCIATION; ANALYTICS; INFORMATION; OPTIMIZATION; CLASSIFIERS; REGRESSION; OBJECTS; SEARCH. With the technology development in cyber physical systems and big data, there are huge potential to apply them to achieve personalization and improve resource efficiency in Industry 4.0. As Industry 4.0 is the relatively new concept originated from an advanced manufacturing vision supported by the German government in 2011, there are only several existing surveys on either cyber physical systems or big data in Industry 4.0. In addition, there are much less surveys related to the intersection between cyber physical systems and big data in Industry 4.0. However, cyber physical systems are closely related to big data in nature. For example, cyber physical systems will continuously generate a large amount of data which requires the big data techniques to process and help to improve system scalability, security, and efficiency. Therefore, we conduct this survey to bring more attention to this critical intersection and highlight the future research direction to achieve the fully autonomy in Industry 4.0.""DPWeVote: differentially private weighted voting protocol for cloud-based decision-making; Industry 4.0; cloud-based design and manufacturing; cloud-based decision-making; weighted voting; differential privacy; RANDOMIZED-RESPONSE. With the advent of Industry 4.0, cloud computing techniques have been increasingly adopted by industry practitioners to achieve better workflows. One important application is cloud-based decision-making, in which multiple enterprise partners need to arrive an agreed decision. Such cooperative decision-making problem is sometimes formed as a weighted voting game, in which enterprise partners express 'YES/NO' opinions. Nevertheless, existing cryptographic approaches to Cloud-Based Weighted Voting Game have restricted collusion tolerance and heavily rely on trusted servers, which are not always available. In this work, we consider the more realistic scenarios of having semi-honest cloud server/partners and assuming maximal collusion tolerance. To resolve the privacy issues in such scenarios, the DPWeVote protocol is proposed which incorporates Randomized Response technique and consists the following three phases: the Randomized Weights Collection phase, the Randomized Opinions Collection phase, and the Voting Results Release phase. Experiments on synthetic data have demonstrated that the proposed DPWeVote protocol managed to retain an acceptable utility for decision-making while preserving privacy in semi-honest environment.""A multiobjective migration algorithm as a resource consolidation strategy in cloud computing; ; PARTICLE SWARM OPTIMIZATION; VIRTUAL MACHINES; SERVER CONSOLIDATION; GENETIC ALGORITHM; ENERGY; MANAGEMENT; POWER; ALLOCATION; PLACEMENT; CENTERS. To flexibly meet users' demands in cloud computing, it is essential for providers to establish the efficient virtual mapping in datacenters. Accordingly, virtualization has become a key aspect of cloud computing. It is possible to consolidate resources based on the single objective of reducing energy consumption. However, it is challenging for the provider to consolidate resources efficiently based on a multiobjective optimization strategy. In this paper, we present a novel migration algorithm to consolidate resources adaptively using a two-level scheduling algorithm. First, we propose the grey relational analysis (GRA) and technique for order preference by similarity to the ideal solution (TOPSIS) policy to simultaneously determine the hotspots by the main selected factors, including the CPU and the memory. Second, a two-level hybrid heuristic algorithm is designed to consolidate resources in order to reduce costs and energy consumption, mainly depending on the PSO and ACO algorithms. The improved PSO can determine the migrating VMs quickly, and the proposed ACO can locate the positions. Extensive experiments demonstrate that the two-level scheduling algorithm performs the consolidation strategy efficiently during the dynamic allocation process.""Will the new technologies turn the page on US productivity growth?; Productivity; Technical change; News shocks; Data mining; . Is the recent slowdown in productivity likely to persist into the foreseeable future? Amazon.com data are used to create updated indicators of technical change and to establish the relationship between them and productivity growth. They reveal a slowdown in innovative activity in the early 2000s but register a recent uptick linked to innovations in robotics, Al, and cloud -computing. Estimates suggest that if these trends were to continue, large productivity gains are likely to be achieved in the near future.""A Cloud-based Architecture for the Cherenkov Telescope Array Observation Simulations: Optimization, Design, and Results; methods: data analysis; methods: numerical; methods: statistical; CTA. Simulating and analyzing detailed observations of astrophysical sources for very high energy experiments, like the Cherenkov Telescope Array (CTA), can be a demanding task especially in terms of CPU consumption and required storage. In this context we propose an innovative cloud computing architecture based on Amazon Web Services (AWS) aiming to decrease the amount of time required to simulate and analyze a given field by distributing the workload and by exploiting the large computational power offered by AWS. We detail how the various services offered by the Amazon online platform are jointly used in our architecture, and we report a comparison of the execution times required for simulating observations of a test source with CTA, by a single machine and the cloud-based approach. We find that, using AWS, we can run our simulations more than 2 orders of magnitude faster than using a general purpose workstation for the same cost. We suggest considering this method when observations need to be simulated, analyzed, and concluded within short timescales.""Determinants of software-as-a-service benefits and impact on firm performance; Software as a Service; Cloud Computing; Governance; Absorptive capacity; Decision support; Firm performance; CLOUD COMPUTING RESEARCH; ABSORPTIVE-CAPACITY; RELATIONAL GOVERNANCE; CAPABILITY; INNOVATION; AGILITY; SUCCESS; MODEL. Software as a Service (SaaS) is increasingly used by firms for sourcing business application software. SaaS can enable a cost reduction and quality improvement of existing operations and provide rapid and low-cost innovation. However, decision makers are unclear about how they can benefit from SaaS. This study contributes to filling this knowledge gap by investigating factors that determine the magnitudes of operational and innovational benefits and firm performance. These research hypotheses were tested using data collected through a survey of 102 Dutch firms that use sophisticated financial SaaS services. The results show that a firm's adaptation to the SaaS model as well as its ACAP positively affects operational and innovation benefits, whereas contractual governance positively affects only the innovational benefits, and relational governance does not affect any of these two types of benefits. Although both operational and innovational benefits positively impact a firm's performance, the former have a stronger impact than the latter. The insights gained from our survey can support firms' decision-making concerning the maximization of the business benefits and firm performance.""Participatory mapping of forest plantations with Open Foris and Google Earth Engine; Crowdsourcing; Planted forests; Open-source; Multi-sensor; Cloud computing; Tanzania; VOLUNTEERED GEOGRAPHIC INFORMATION; DECIDUOUS RUBBER PLANTATIONS; LAND-COVER; INTEGRATING PALSAR; TROPICAL FORESTS; GLOBAL CROPLAND; MODIS; CHINA; MAPS. Recent years have witnessed the practical value of open-access Earth observation data catalogues and software in land and forest mapping. Combined with cloud-based computing resources, and data collection through the crowd, these solutions have substantially improved possibilities for monitoring changes in land resources, especially in areas with difficult accessibility and data scarcity. In this study, we developed and tested a participatory mapping methodology utilizing the open data catalogues and cloud computing capacity to map the previously unknown extent and species composition of forest plantations in the Southern Highlands area of Tanzania, a region experiencing a rapid growth of smallholder-owned woodlots. A large set of reference data, focusing on forest plantation coverage, species and age information distribution, was collected in a two-week participatory GIS campaign where 22 Tanzanian experts interpreted very high-resolution satellite images in Google Earth with the Open Foris Collect Earth tool developed by the Food and Agriculture Organization of the United Nations. The collected samples were used as training data to classify a multi-sensor image stack of Landsat 8 (2013-2015), Sentinel-2 (2015-2016), Sentinel-1 (2015), and SRTM derived elevation and slope data layers into a 30 m resolution forest plantation map in Google Earth Engine. The results show that the forest plantation area was estimated with high overall accuracy (85%). The interpretation accuracy of local experts was high considering general definition of forest plantation declining with increased details in interpretation attributes. The results showcase the unique value of local expert participation, enabling the collection of thousands of reference samples over a large geographical area in a short period of time simultaneously building the capacity of the experts. However, sufficient training prior to the data collection is crucial for the interpretation success especially when detailed interpretation is conducted in complex landscapes. Since the methodology is built on open-access data and software, it presents a highly feasible solution for repetitive land resource mapping applicable at different spatial scales globally.""Electrochemical immunosensor utilizing electrodeposited Au nanocrystals and dielectrophoretically trapped PS/Ag/ab-HSA nanoprobes for detection of microalbuminuria at point of care; Electrochemical immunosensor; Microalbuminuria; Interdigitated Microelectrodes; Nanoprobes; Dielectrophoresis; NANOPARTICLES; IMMUNOASSAYS; ALBUMINURIA. In this study, we have fabricated a simple disposable electrochemical immunosensor for the point of care testing of microalbuminuria, a well-known clinical biomarker for the onset of chronic kidney disease. The immunosensor is fabricated by screen-printing carbon interdigitated microelectrodes on a flexible plastic substrate and utilizes electrochemical impedance spectroscopy to enable direct and label free immunosensing by analyzing interfacial changes on the electrode surface. To improve conductivity and biocompatibility of the screen-printed electrodes, we have modified it with gold nanoparticles, which are electrodeposited using linear sweep voltammetry. To enable efficient immobilization of HSA antibodies, we have developed novel PS/Ag/ab-HSA nanoprobes (polystyrene nanoparticle core with silver nanoshells covalently conjugated to HSA antibodies), and these nanoprobes are trapped on the electrode surface using dielectrophoresis. Each immunosensor has two sensing sites corresponding to test and control to improve specificity by performing differential analysis. Immunosensing results show that the normalized impedance response is linearly dependent on albumin concentration in the clinically relevant range with good repeatability. We have also developed a portable impedance readout module that can analyze the data obtained from the immunosensor and transmit it wirelessly for cloud computing. Consequently, the developed immunosensing platform can be extended to the detection of a range of immunoreactions and shows promise for point of diagnosis and public healthcare monitoring.""Data locality optimization based on data migration and hotspots prediction in geo-distributed cloud environment; Geo-distributed cloud; Data locality; Data migration; Hotspots prediction; MAPREDUCE. With the explosive growth of data-intensive mobile, social, commercial and industrial applications. geo-distributed cloud becomes the main trend of cloud computing due to its advantages of higher flexible scalability, stronger stability, lower latency, and more diverse services. Due to the limited network band-width, communication across geographic data centers typically suffers from wide-area latencies, which significantly deteriorates system performance. Data locality is an effective way to solve this problem. In order to provide flexible cloud computing services for data-intensive applications, combining with the advantage of geo-distributed cloud computing paradigm, this paper proposed a data locality optimization method based on data migration (DLO-Migrate) and a data locality optimization algorithm based on hotspots prediction (DLO-Predict) to reduce data access delay in geo-distributed cloud environment. In DLO-Migrate method, tasks are assigned according to node locality, and access data of non-node-locality tasks are migrated in advance by using the idle network bandwidth. In DLO-Predict algorithm, from cloud-level data locality perspective, hot files are predicted and synchronized periodically among data centers of the geo-distributed cloud during information interaction. Extensive experimental results show that, compared with baseline algorithms, our proposed algorithms can improve data locality of geo-distributed cloud and reduce job completion time substantially.""VBTree: forward secure conjunctive queries over encrypted data for cloud computing; Privacy preserving; Cloud computing; Searchable symmetric encryption; SEARCHES. This paper concerns the fundamental problem of processing conjunctive keyword queries over an outsourced data table on untrusted public clouds in a privacy-preserving manner. The data table can be properly implemented with tree-based searchable symmetric encryption schemes, such as the known Keyword Red-Black tree and the Indistinguishable Bloom-filter Tree in ICDE'17. However, as for these trees, there still exist many limitations to support sub-linear time updates. One of the reasons is that their tree branches are directly exposed to the cloud. To achieve efficient conjunctive queries while supporting dynamic updates, we introduce a novel tree data structure called virtual binary tree (VBTree). Our key design is to organize indexing elements into the VBTree in a top-down fashion, without storing any tree branches and tree nodes. The tree only exists in a logical view, and all of the elements are actually stored in a hash table. To achieve forward privacy, which is discussed by Bost in CCS'16, we also propose a storage mechanism called version control repository (VCR), to record and control versions of keywords and queries. VCR has a smaller client-side storage compared to other forward-private schemes. With our proposed approach, data elements can be quickly searched while the index can be privately updated. The security of the VBTree is formally proved under the IND-CKA2 model. We test our scheme on a real e-mail dataset and a user location dataset. The testing results demonstrate its high efficiency and scalability in both searching and updating processes.""FSCRank: A Failure-Sensitive Structure-Based Component Ranking Approach for Cloud Applications; component ranking; failure impact; application structure; buffer node; availability improvement; CENTRALITY. Cloud computing has attracted a growing number of enterprises to move their business to the cloud because of the associated operational and cost benefits. Improving availability is one of the major concerns of cloud application owners because modern applications generally comprise a large number of components and failures are common at scale. Fault tolerance enables an application to continue operating properly when failure occurs, but fault tolerance strategy is typically employed for the most important components because of financial concerns. Therefore, identifying important components has become a critical research issue. To address this problem, we propose a failure-sensitive structure-based component ranking approach (FSCRank), which integrates component failure impact and application structure information into component importance evaluation. An iterative ranking algorithm is developed according to the structural characteristics of cloud applications. The experimental results show that FSCRank outperforms the other two structure-based ranking algorithms for cloud applications. In addition, factors that affect application availability optimization are analyzed and summarized. The experimental results suggest that the availability of cloud applications can be greatly improved by implementing fault tolerance strategy for the important components identified by FSCRank.""Near real-time agriculture monitoring at national scale at parcel resolution: Performance assessment of the Sen2-Agri automated system in various cropping systems around the world; Agriculture monitoring; Cloud computing; Machine learning; Sentinel-2; Crop type mapping; Cropland; EARTH OBSERVATION REQUIREMENTS; BIDIRECTIONAL REFLECTANCE; CANOPY; SET; EXTRACTION; SENESCENCE; VARIABLES; MODELS; SEASON. The convergence of new EO data flows, new methodological developments and cloud computing infrastructure calls for a paradigm shift in operational agriculture monitoring. The Copernicus Sentinel-2 mission providing a systematic 5-day revisit cycle and free data access opens a completely new avenue for near real-time crop specific monitoring at parcel level over large countries. This research investigated the feasibility to propose methods and to develop an open source system able to generate, at national scale, cloud-free composites, dynamic cropland masks, crop type maps and vegetation status indicators suitable for most cropping systems. The so-called Sen2-Agri system automatically ingests and processes Sentinel-2 and Landsat 8 time series in a seamless way to derive these four products, thanks to streamlined processes based on machine learning algorithms and quality controlled in situ data. It embeds a set of key principles proposed to address the new challenges arising from countrywide 10 m resolution agriculture monitoring. The full-scale demonstration of this system for three entire countries (Ukraine, Mali, South Africa) and five local sites distributed across the world was a major challenge met successfully despite the availability of only one Sentinel-2 satellite in orbit. In situ data were collected for calibration and validation in a timely manner allowing the production of the four Sen2-Agri products over all the demonstration sites. The independent validation of the monthly cropland masks provided for most sites overall accuracy values higher than 90%, and already higher than 80% as early as the mid-season. The crop type maps depicting the 5 main crops for the considered study sites were also successfully validated: overall accuracy values higher than 80% and F1 Scores of the different crop type classes were most often higher than 0.65. These respective results pave the way for countrywide crop specific monitoring system at parcel level bridging the gap between parcel visits and national scale assessment. These full-scale demonstration results clearly highlight the operational agriculture monitoring capacity of the Sen2-Agri system to exploit in near real-time the observation acquired by the Sentinel-2 mission over very large areas. Scaling this open source system on cloud computing infrastructure becomes instrumental to support market transparency while building national monitoring capacity as requested by the AMIS and GEOGLAM G-20 initiatives.""Towards virtual machine introspection based security framework for cloud; Virtual Machine Introspection; cloud computing; vector space model; system call trace; malware; INTRUSION DETECTION; CLASSIFICATION. Virtualization enables provision of resources to users according to their requirement through Infrastructure as a Service (IaaS) delivery model in cloud computing environment. Malicious users could lease cloud resources and use them as platforms to launch attacks. In this paper, we propose a Virtual Machine Introspection (VMI)-based security framework to monitor cloud users' in-VM activities and detect malicious one if any. We justify our selection of VMI method based on hardware knowledge for proposed framework by discussing its key advantages over other VMI methods. We propose design of multi-threaded analysis component that can introspect number of virtual machines hosted on cloud servers in real time. Experimental results demonstrate that our framework performs well with a set of metrics appropriate for cloud IaaS environment.""Optimal fitness aware cloud service composition using modified invasive weed optimization; Quality of Service (QoS); Cloud service composition; Meta-heuristics; Invasive weed optimization; Fitness metrics; PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; SELECTION; PARADIGM. Quality of Service (QoS)-aware cloud service composition is one of the pivotal problems in cloud computing. With the seamless proliferation of cloud services, it becomes challenging to obtain an optimal cloud service for composition that satisfies a user's requirements. Many composition models available in the literature compose cloud services based on one or two QoS parameters of the candidate services without considering the complete set. These composition models do not consider the connectivity constraints between the candidate cloud services for satisfying a workflow/function in a service composition. In this paper, we present a novel Optimal Fitness Aware Cloud Service Composition using Modified Invasive Weed Optimization dealing with multiple QoS parameters and satisfying the balancing of QoS parameters and the connectivity constraints of cloud service composition. We evaluate the performance of our approach on a data set of real world cloud services, to select the best optimal fitness aware cloud service composition. By performing the parametric and non-parametric test at 1% level of significance, our proposed method is statistically more accurate than the other methods compared.""A prediction-based and power-aware virtual machine allocation algorithm in three-tier cloud data centers; cloud computing; dynamic consolidation; power efficiency; three-tier cloud data centers; virtual machine allocation; virtualization; ENERGY EFFICIENCY; DYNAMIC CONSOLIDATION; RESOURCE-MANAGEMENT; PERFORMANCE; HEURISTICS; CONSUMPTION; COST. With the increasing popularity of cloud computing services, the more number of cloud data centers are constructed over the globe. This makes the power consumption of cloud data center elements as a big challenge. Hereby, several software and hardware approaches have been proposed to handle this issue. However, this problem has not been optimally solved yet. In this paper, we propose an online cloud resource management with live migration of virtual machines (VMs) to reduce power consumption. To do so, a prediction-based and power-aware virtual machine allocation algorithm is proposed. Also, we present a three-tier framework for energy-efficient resource management in cloud data centers. Experimental results indicate that the proposed solution reduces the power consumption; at the same time, service-level agreement violation (SLAV) is also improved.""Privacy-preserving public auditing for secure data storage in fog-to-cloud computing; Public auditing; Data storage; Privacy protection; Data integrity; Fog-to-cloud computing; Internet of things; EFFICIENT; INTERNET; THINGS. With increasing popularity of fog-to-cloud based Internet of Things (IoT), how to ensure the integrity of IoT data outsourced in clouds has become one of the biggest security challenges. However, little effort has been put into addressing the problem. To fill this gap, this paper presents a tailor-made public auditing scheme for data storage in fog-to-cloud based IoT scenarios, which can achieve all indispensable performance and security requirements. Particularly, we design a tag-transforming strategy based on the bilinear mapping technique to convert the tags generated by mobile sinks to the ones created by the fog nodes in the phase of proof generation, which cannot only effectively protect the identity privacy, but also reduce the communication and computational costs in the verification phase; moreover, we present a zero-knowledge proof mechanism to verify the integrity of IoT data from various generators (e.g., mobile sinks and fog nodes) while achieving perfect data-privacy preserving. We formally prove the security of our scheme and evaluate its performance by theoretical analysis and comprehensive experiments. The results demonstrate that our scheme can efficiently achieve secure auditing for data storage in fog-to-cloud based IoT scenarios, and outperforms the straight-forward solution in communication and computational costs as well as energy consumption.""Network selection criterion for ubiquitous communication provisioning in smart cities for smart energy system; Smart meter; Smart grid; Smart cities; IoT; ICT; HAN; NAN; WAN; GRID APPLICATIONS; POWER-LINE; TECHNOLOGIES; ARCHITECTURES; CHALLENGES; MANAGEMENT. With the expansion of urbanization, energy becomes one of the foremost requirements of a smart city. Low energy consumption, renewable energy, and carbon footprints reduction are prime targets of a smart city. A smart city needs to be energy efficient and technology driven. Information and Communication Technology (ICT) helps to improve system efficiency and creates a way for the end consumer to communicate with utilities and the network service provider. The two-way communication model of a smart grid makes possible reliable, stable and efficient communication between utility and consumer by using ICT. The major issue in smart energy system is the lack of necessary interoperability and integration of communication standards, which hampers the effective deployment of communication networks. In this paper, we present a hierarchical architecture to resolve this issue, which will support smart energy system infrastructure and services in smart cities for provisioning of ubiquitous communication. It consists of two computing zones (fog computing and cloud computing) along with two implementation phases (initial and final). Fog computing will help to offer low latency response to anomalous and hazardous events in real time while reducing a burden on the cloud for computing as well as storage. To provide ubiquitous communication in smart energy system for data acquisition, a network selection algorithm has been proposed. TOPSIS MADM technique is employed for network selection in a heterogeneous environment at HAN/NAN (Bluetooth, Zigbee, Z-Wave, WLAN, LoRaWAN) and WAN (GPRS, UMTS, LTE, and WiMAX) levels of smart energy system's multilayer communication infrastructure. Coverage area, data rate, power consumption and security level attributes are employed for network selection. The results revealed that WLAN performed better in case of HAN/NAN environment, whereas LTE worked best at WAN level.""Joint medical image compression-encryption in the cloud using multiscale transform-based image compression encoding techniques; Cloud computing; RSA; bandelet; wavelet; curvelet; countourlet; ridgelet; SPIHT; EZW; WDR; ASWDR; CONTOURLET TRANSFORM; WAVELET TRANSFORM; SECURITY; KEY. The recent years have witnessed rapid strides in the use of cloud computing and its countless applications. A cloud can contain massive volumes of multimedia data in the form of images, video and audio. Cloud computing platforms confront challenges in terms of data confidentiality, message integrity, user authentication and compression. Multimedia data needs plenty of storage capacity. Consequently, there is a need for multimedia data compression to reduce data size. Compression techniques are quite reliable, offering benefits to organizations dealing with metasized data in the cloud. Compressing large quanta of data leads to superior utilization of cloud storage. Compression techniques can compress data used for storage and transmission, yet compression alone is inadequate because multimedia data shared should, of necessity, be secure. Therefore, both multimedia compression and security are mandatory in the cloud. The chief goal of this paper is to propose a new framework, comprising multiscale transforms, public key cryptography and appropriate encoding techniques, that performs joint medical image compression and image encryption in the cloud. Multiscale transforms play a lead role in image compression, and the ones discussed in this paper include wavelet, bandelet, curvelet, ridgelet and contourlet transforms. Wavelet transforms offer robust localization both in terms of time and frequency domains. Bandelet transforms offer natural images geometric regularity to help improve the efficiency of representation. Curvelet transforms handle curve discontinuities well, with ridgelet transforms being the core idea behind curvelets. Contourlet transforms capture smooth contours and edges at any orientation. The Rivest-Shamir-Adleman (RSA) algorithm is used to encrypt images to provide maximum security when they are being transferred. Encoding techniques involved in this paper comprise the Embedded Zerotree Wavelet (EZW), Set Partitioning in Hierarchical Trees (SPIHT), Wavelet Difference Reduction (WDR), and Adaptively Scanned Wavelet Difference Reduction (ASWDR). Performance parameters such as peak signal to noise ratio (PSNR), mean square error (MSE), image quality index and structural similarity index (SSIM) are used for evaluation. It is justified that the proposed framework compresses images securely in the cloud.""Energy-Efficient Dynamic Computation Offloading and Cooperative Task Scheduling in Mobile Cloud Computing; Mobile cloud computing; energy-efficiency cost; computation offloading; resource allocation; FRAMEWORK. Mobile cloud computing (MCC) as an emerging and prospective computing paradigm, can significantly enhance computation capability and save energy for smart mobile devices (SMDs) by offloading computation-intensive tasks from resource-constrained SMDs onto resource-rich cloud. However, how to achieve energy-efficient computation offloading under hard constraint for application completion time remains a challenge. To address such a challenge, in this paper, we provide an energy-efficient dynamic offloading and resource scheduling (eDors) policy to reduce energy consumption and shorten application completion time. We first formulate the eDors problem into an energy-efficiency cost (EEC) minimization problem while satisfying task-dependency requirement and completion time deadline constraint. We then propose a distributed eDors algorithm consisting of three subalgorithms of computation offloading selection, clock frequency control, and transmission power allocation. Next, we show that computation offloading selection depends on not only the computing workload of a task, but also the maximum completion time of its immediate predecessors and the clock frequency and transmission power of the mobile device. Finally, we provide experimental results in a real testbed and demonstrate that the eDors algorithm can effectively reduce EEC by optimally adjusting CPU clock frequency of SMDs in local computing, and adapting the transmission power for wireless channel conditions in cloud computing.""Brain Storm Optimization Graph Theory (BSOGT) and Energy Resource Aware Virtual Network Mapping (ERVNM) for Medical Image System in Cloud; Virtual Network Mapping (VNM); Brain Storm Optimization Graph Theory (BSGOT); Virtualization quality of services (Qos); Distributed cloud computing and optimization; MANAGEMENT. With the development of Internet and the make use of Internet for medical information, the demand for huge scale and reliable managing medical information has brought out the huge scale Internet data centers. This work that has been presented here highlights the structural lay out and formulation of the medical information model. The aim of presenting this to aid medical departments as well as workers to exchange information and integrate available resources that help facilitate the analysis to be conducted on the given information. Software here comprises of medical information and offers a comprehensive service structure that benefits medical data centers. VNM or Virtual Network Mapping (VNM) essentially relates to substrate network that involves the installation and structuring of on demand virtual machines. These however are subjective to certain limitations that are applicable in relation to latency, capacity as well as bandwidth. Data centers need to dynamically handle cloud workloads effectively and efficiently. Simultaneously, since the mapping of virtual and physical networks with several providers' consumes more time along with energy. In order to resolve this issue, VNM has been mapped by making use of Graph Theory (GT) matching, a well-studied database topic. (i) Brain Storm Optimization Graph Theory (BSOGT) is introduced for modeling a virtual network request in the form of a GT with different resource constraints, and the substrate networks here is considered being a graph. For this graph the nodes and edges comprise of attributes that indicate their constraints. (ii) The algorithm that has been recently introduced executes graph decomposition into several topology patterns. Thereafter the BSOGT is executed to solve any issues that pertain to mapping. (iii) The model that has been presented here, ERVNM and the BSOGT are used with a specific mapping energy computation function.(iv) Issues pertaining to these are categorized as being those related to virtual network mapping as the ACGT and optimal solution are drawn by using effective integer linear programming. ACGT, pragmatic approach, as well as the precise and two-stage algorithms performance is evaluated by means of cloud Simulator environment. The results obtained from simulation indicate that the BSOGT algorithm attains the objectives of cloud service providers with respect to Acceptance ratio, mapping percentage, processing time as well as Convergence Time.""ASL-MRICloud: An online tool for the processing of ASL MRI data; arterial spin labeling; ASL-MRICloud; cerebral blood flow; cloud computing; data analysis; CEREBRAL-BLOOD-FLOW; NONLINEAR MINIMIZATION SUBJECT; ALZHEIMERS-DISEASE; ARTERIAL; QUANTIFICATION; OPTIMIZATION; SOFTWARE. Arterial spin labeling (ASL) MRI is increasingly used in research and clinical settings. The purpose of this work is to develop a cloud-based tool for ASL data processing, referred to as ASL-MRICloud, which may be useful to the MRI community. In contrast to existing ASL toolboxes, which are based on software installation on the user's local computer, ASL-MRICloud uses a web browser for data upload and results download, and the computation is performed on the remote server. As such, this tool is independent of the user's operating system, software version, and CPU speed. The ASL-MRICloud tool was implemented to be compatible with data acquired by scanners from all major MRI manufacturers, is capable of processing several common forms of ASL, including pseudo-continuous ASL and pulsed ASL, and can process single-delay and multi-delay ASL data. The outputs of ASL-MRICloud include absolute and relative values of cerebral blood flow, arterial transit time, voxel-wise masks indicating regions with potential hyper-perfusion and hypo-perfusion, and an image quality index. The ASL tool is also integrated with a T-1-based brain segmentation and normalization tool in MRICloud to allow generation of parametric maps in standard brain space as well as region-of-interest values. The tool was tested on a large data set containing 309 ASL scans as well as on publicly available ASL data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.""Toward a unified framework for Cloud Computing governance: An approach for evaluating and integrating IT management and governance models; Cloud Computing; Cloud Computing governance; Unified framework; IT models evaluation; IT models integration; HARMONIZATION; STANDARDS. Cloud Computing is currently one of the major trends in the computer industry. It offers a wide range of both opportunities and challenges. The lack of governance causes a slowdown in the adoption process, especially with an absence of a comprehensive approach supporting the Cloud Computing governance objectives. Regarding this issue, our goal is to develop a new management and governance framework based on endorsed IT models; namely ITIL, COBIT, and ISO/IEC 27001/2. These models have been developed for different purposes and aspects of IT governance. The proposed framework is an attempt to a unified approach by taking into consideration the models' dissimilarities. Before we go further with the final unification stage, this study's main objective is to develop both the evaluation and the integration approaches with regwards to the IT models. The results, therefore, are to shed light on the relevant outcomes emanate from an elaboration within an entirely unified CC governance framework.""CBase: Fast Virtual Machine storage data migration with a new data center structure; Cloud computing; Data center; Live VM migration; Storage data migration; Data deduplication; P2P file sharing; CLOUD; DEDUPLICATION. Live Virtual Machine (VM) migration within a data center is an important technology for cloud management, and has brought many benefits to both cloud providers and users. With the development of cloud computing, across-data-center VM migration is also desired. Normally, there is no shared storage system between data centers, hence the storage data (disk image) of a VM will be migrated to the destination data center as well. However, the slow network speed of the Internet and the comparatively large size of VM disk image make VM storage data migration become a bottleneck for live VM migration across data centers. In this paper, based on a detailed analysis of VM deployment models and the nature of VM image data, we design and implement a new migration system, called CBase. The key concept of CBase is a newly introduced central base image repository for reliable and efficient data sharing between VMs and data centers. With this central repository, further performance optimizations to VM storage data migration are made possible. Two migration mechanisms (data deduplication and Peer-to-Peer (P2P) file sharing) are utilized to accelerate base image migration, and a strategy is designed to elevate the synchronization of newly-written disk blocks. The results from an extensive experiment show that CBase significantly outperforms existing migration mechanisms under different conditions regarding total migration time and total network traffic. In particular, CBase with data deduplication is better than P2P file sharing for base image migration in our experimental environment.""Anti-spoofing cloud-based multi-spectral biometric identification system for enterprise security and privacy-preservation; Cloud computing; Multi-spectral biometric identification; Enterprise security and; privacy-preservation; Palmprint biometric trait; RSA algorithm; REPRESENTATION; RECOGNITION; PROTECTION; ALGORITHM. Nowadays, cloud computing has provided enterprises and users with several capabilities to process and store their data in various cloud data centers. Storing and processing these sensitive data with better protection and management are a big challenge. Therefore, there is a need for maintaining the confidentiality and integrity of data in the cloud without any information leakage. Recently, biometric recognition has achieved significant advancements in the identification of individuals for the purpose of privacy preservation in the cloud computing. Only few works have used a face as a typical biometric trait for cloud and cross-enterprise identification in the last recent years. However, current cloud-based biometric identification systems and approaches have some limitations such as noisy data, inter and intra class variations, high time cost, inaccurate, non-universality and spoofing attack. This paper proposes a new anti-spoof multispectral biometric cloud-based identification approach for privacy and security of cloud computing. The approach offers the solution using multi-spectral palmprint as a typical biometric trait between two main phases: offline enrollment phase and online identification phase. This work is considered the first approach of privacy-preservation in cloud computing using encrypted multi-spectral palmprint features without any information leakage and disclosure possibility. The experimental results show that the proposed approach can accurately and efficiently provide the privacy and security of cloud data.""Secure authentication and load balancing of distributed edge datacenters; Edge computing; Fog computing; Edge datacenter; Cloud; Security; Authentication; Load balancing; CLOUD; INTERNET. Edge computing is an emerging research area to incorporate cloud computing into edge network devices. An Edge datacenter, also referred to as EDC, processes data streams and user requests in real-time and is therefore used to decrease the latency and congestion in the network, EDC is usually setup as a distributed system and is accordingly placed between the cloud datacenter and the data source. These EDCs work as an intermediate layer in the fog hierarchy between IoT and Cloud datacenter. EDC's are aided by load balancers, responsible for distributing the workload amongst multiple EDC, in order to optimize resource utilization and response time. The load balancers make sure that the workload is equally divided amongst the available EDCs to avoid over loading of some EDCs while other remain idle as this directly impacts the user response and real-time event detection. Given the fact that EDCs are deployed in remote environments, the need for secure authentication is of major importance. In this paper we propose a novel load balancing technique that enables EDC authentication as well as identification of idle EDCs for better load balancing. The proposed load balancing technique is also compared with existing approaches and proves to be more efficient in locating EDC's with less workload. In addition to the improved efficiency, the proposed scheme also strengthens the security of the network by incorporating destination EDC authentication. Crown Copyright""Secure Multi-Party Computation: Theory, practice and applications; Secure Multi-Party Computation; Generic protocol; Cloud security; Secure outsourcing; Privacy-preserving technology; PRIVACY. Secure Multi-Party Computation (SMPC) is a generic cryptographic primitive that enables distributed parties to jointly compute an arbitrary functionality without revealing their own private inputs and outputs. Since Yao's seminal work in 1982, 30 years of research on SMPC has been conducted, proceeding from pure theoretical research into real-world applications. Recently, the increasing prevalence of the newly emerging technologies such as cloud computing, mobile computing and the Internet of Thing has resulted in a re-birth of SMPC's popularity. This has occurred mainly because, as a generic tool for computing on private data, SMPC has a natural advantage in solving security and privacy issues in these areas. Accordingly, many application-oriented SMPC protocols have been constructed. This paper presents a comprehensive survey on the theoretical and practical aspects of SMPC protocols. Specifically, we start by demonstrating the underlying concepts of SMPC, including its security requirements and basic construction techniques. Then, we present the research advances regarding construction techniques for generic SMPC protocols, and also the cutting-edge approaches to cloud-assisted SMPC protocols. Then, we summarize the concrete application-oriented protocols that are currently available, and finally, we present a discussion of the current literature and conclude this survey."