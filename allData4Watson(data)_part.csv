"Postpartum depression prediction through pregnancy data analysis for emotion-aware smart systems updates; Emotion-aware smart systems; Cognitive computing; Information fusion; Mental health solutions; Maternal care quality improvement; SENSOR NETWORKS; CLASSIFICATION; RECOGNITION; ALGORITHMS; INTERNET; ANXIETY; FUSION; RISK. Emotion-aware computing represents an evolution in machine learning enabling systems and devices process to interpret emotional data to recognize human behavior changes. As emotion-aware smart systems evolve, there is an enormous potential for increasing the use of specialized devices that can anticipate life-threatening conditions facilitating an early response model for health complications. At the same time, applications developed for diagnostic and therapy services can support conditions recognition (as depression, for instance). Hence, this paper proposes an improved algorithm for emotion-aware smart systems, capable for predicting the risk of postpartum depression in women suffering from hypertensive disorders during pregnancy through biomedical and sociodemographic data analysis. Results show that ensemble classifiers represent a leading solution concerning predicting psychological disorders related to pregnancy. Merging novel technologies based on IoT, cloud computing, and big data analytics represent a considerable advance in monitoring complex diseases for emotion aware computing, such as postpartum depression.""Verifiable outsourced computation over encrypted data; Outsourced computation; Verifiability; Privacy preservation; Fully homomorphic encryption; Polynomial factorization algorithm; . In recent years, cloud computing has become the most popular and promising service platform. A cloud user can outsource its heavy computation overhead to a cloud service provider (CSP) and let the CSP make the computation instead. In order to guarantee the correctness of the outsourced processing (e.g., machine learning and data mining), a proof should be provided by the CSP in order to make sure that the processing is carried out properly. On the other hand, from the security and privacy points of view, users will always encrypt their sensitive data first before they are outsourced to the CSP rather than sending the raw data directly. However, processing and verifying of encrypted data computation has always been a challenging problem. Homomorphic Encryption (HE) has been proposed to tackle this task on computations over encrypted data and ensure the confidentiality of the data. However, original HE cannot provide an efficient approach to verify the correctness of computation over encrypted data that is processed by CSP. In this paper, we propose a verifiable outsourced computation scheme over encrypted data with the help of fully homomorphic encryption and polynomial factorization algorithm. Our scheme protects user data security in outsourced processing and allows public verification on the computation result processed by CSP with zero knowledge. We then prove the security of our scheme and analyze its performance by comparing it with some latest related works. Performances analysis shows that our scheme reduces the overload of both the cloud users and the verifier.""Secure weighted possibilistic c-means algorithm on cloud for clustering big data; Big data; Possibilistic c-means algorithm; Cloud computing; BGV; . The weighted possibilistic c-means algorithm is an important soft clustering technique for big data analytics with cloud computing. However, the private data will be disclosed when the raw data is directly uploaded to cloud for efficient clustering. In this paper, a secure weighted possibilistic c-means algorithm based on the BGV encryption scheme is proposed for big data clustering on cloud. Specially, BGV is used to encrypt the raw data for the privacy preservation on cloud. Furthermore, the Taylor theorem is used to approximate the functions for calculating the weight value of each object and updating the membership matrix and the cluster centers as the polynomial functions which only include addition and multiplication operations such that the weighed possibilistic c-means algorithm can be securely and correctly performed on the encrypted data in cloud. Finally, the presented scheme is estimated on two big datasets, i.e., eGSAD and sWSN, by comparing with the traditional weighted possibilistic c-means method in terms of effectiveness, efficiency and scalability. The results show that the presented scheme performs more efficiently than the traditional weighted possiblistic c-means algorithm and it achieves a good scalability on cloud for big data clustering.""New publicly verifiable computation for batch matrix multiplication; Publicly verifiable computation; Public delegation; Batch matrix multiplication; Privacy protection; LARGE-SCALE SYSTEMS; CLOUD; SECURE; EFFICIENT; SERVICE; ALGORITHMS; DELEGATION; DATABASE. With the prevalence of cloud computing, the resource constrained clients are trended to outsource their computation-intensive tasks to the cloud server. Although outsourcing computation paradigm brings many benefits for both clients and cloud server, it causes some security challenges. In this paper, we focus on the outsourcing computation of matrix multiplication, and propose a new publicly verifiable computation scheme for batch matrix multiplication. Different from traditional matrix computation outsourcing model, the outsourcing task of our scheme is to compute MXi for group of clients, where X-i is a private matrix chosen by different clients and M is a public matrix given by a data center beforehand. Based on the two techniques of privacy-preserving matrix transformation and matrix digest, our scheme can protect the secrecy of the client's private matrix X-i and dramatically reduce the computation cost in both the key generation and the computing phases. Security analysis shows that the proposed scheme can achieve the desired security properties under the co-computational Diffie-Hellman assumption. We also provide the experimental evaluation that demonstrates the efficiency of our scheme.""Simulating large vCDN networks: A parallel approach; vCDN; Simulation; VM placement; OpenMP; . Virtualization and cloud computing are being used by Communication Service Providers to deploy and utilize virtual Content Distribution Networks (vCDNs) to reduce costs and increase elasticity thereby avoiding performance, quality, reliability and availability limitations that characterize traditional CDNs. As cache placement is based on both the content type and geographic location of a user request, it has a significant impact on service delivery and network congestion. To study the effectiveness of cache placements and hierarchical network architectures composed of sites, a novel parallel simulation framework is proposed utilizing a discretetime approach. Unlike other simulation approaches, the proposed simulation framework can update, in parallel, the state of sites and their resource utilization with respect to incoming requests in a significantly faster manner at hyperscale. It allows for simulations with multiple types of content, different virtual machine distributions, probabilistic caching, and forwarding of requests. In addition, power consumption models allow the estimation of energy consumption of the physical resources that host virtual machines. The results of simulations conducted to assess the performance and applicability of the proposed simulation framework are presented. Results are promising for the potential of this simulation framework in the study of vCDNs and optimization of network infrastructure.""Cloud computing for seizure detection in implanted neural devices; implantable neural stimulators; cloud computing; epilepsy; iEEG; seizure detection; RESPONSIVE CORTICAL STIMULATION; BRAIN-STIMULATION; ELECTRICAL-STIMULATION; EPILEPTIC SEIZURES; LONG-TERM; ONSET; NEUROSTIMULATION; PREDICTION; SECURITY; PRIVACY. Objective. Closed-loop implantable neural stimulators arc an exciting treatment option for patients with medically refractory epilepsy, with a number of new devices in or nearing clinical trials. These devices must accurately detect a variety of seizure types in order to reliably deliver therapeutic stimulation. While effective, broadly-applicable seizure detection algorithms have recently been published, these methods arc too computationally intensive to be directly deployed in an implantable device. We demonstrate a strategy that couples devices to cloud computing resources in order to implement complex seizure detection methods on an implantable device platform. Approach. We use a sensitive gating algorithm capable of running on-board a device to identify potential seizure epochs and transmit these epochs to a cloud-based analysis platform. A precise seizure detection algorithm is then applied to the candidate epochs, leveraging cloud computing resources for accurate seizure event detection. This seizure detection strategy was developed and tested on eleven human implanted device recordings generated using the NeuroVista Seizure Advisory System. Main results. The gating algorithm achieved high-sensitivity detection using a small feature set as input to a linear classifier, compatible with the computational capability of next-generation implantable devices. The cloud-based precision algorithm successfully identified all seizures transmitted by the gating algorithm while significantly reducing the false positive rate. Across all subjects, this joint approach detected 99% of seizures with a false positive rate of 0.03 h(-1). Significance. We present a novel framework for implementing computationally intensive algorithms on human data recorded from an implanted device. By using telemetry to intelligently access cloud-based computational resources, the next generation of neuro-implantable devices will leverage sophisticated algorithms with potential to greatly improve device performance and patient outcomes.""Practical public key encryption with selective opening security for receivers; Data security; Selective opening attacks; Chosen-ciphertext attacks; Receivers; PROTECTION; EFFICIENT. Data security and privacy protection is one of the highest concerns in cloud computing. Selective opening security (SOA security) is a security notion focusing oil a multi-user setting on the Internet. In recent years, many public-key encryption (PKE) constcutions have been proposed that meet SOA security. Most of them focus on the sender corruption setting. However, less attention has been paid on the receiver corruption setting. Inspired by the work of Heuer et al. (PKC 2015), which showed a practical SOA secure PKE scheme (in the sender corruption setting) from a key encapsulation mechanism (KEM) and a message authentication code (MAC), we propose a practical generic PKE construction achieving simulation-based SOA security under chosen-ciphertext attacks for recievers (RSIM-SO-CCA security). Our construction is also based on a KEM and a MAC. We show that if the underlying KEM is one-way secure in the presence of a plaintext-checking oracle (OW-PCA) and the underlying MAC is strong unforgeable under one-time chosen message attacks (sUF-OT-CMA), then our generic construction is RSIM-SO-CCA secure in the random oracle model.""Crosstalk-aware routing, spectrum, mode and time assignment using FMF with partial MIMO equalization in flexible grid datacenter networks; Datacenter network (DCN); Elastic optical network (EON); Spatial division multiplexing (SDM); Few mode fiber (FMF); Routing, spectrum, mode and time assignment (RSMTA); WAVELENGTH; ALLOCATION. Recently, lots of datacenters (DCs) have been built to enhance reliability and expand processing ability for computing service in cloud computing. Furthermore, spatial division multiplexing (SDM) technology and elastic optical network (EON) are also considered as the candidate solutions to solve the requirements of huge bandwidth and flexible connections. However, it is much more difficult to use few mode fiber (FMF) in DCs-based EON due to the existing modal crosstalk among different modes. Faced with the challenges, we propose a routing, spectrum, mode and time assignment algorithm to decrease the blocking probability and increase the spectrum utilization. To deal with the problem more efficiently, we propose a method called flexible sliding three-dimensional block (3D block) which represents the dimensions of spectrum, space and time in our algorithm. 3D block slides in the resource space to find out proper resources. The simulations are conducted on datacenter networks (DCNs) and the results show that lower blocking probability and higher spectrum utilization are achieved by efficiently utilizing the spectrum, space and time domains.""Guarded dual authentication based DRM with resurgence dynamic encryption techniques; Twofold authentication protocol; Efficient Elliptic Curve Public Key Encryption (EECPKE); integrity checking protocol; Covariance matrix Adaptation Evolution Strategies (CMA-ES); PROXY RE-ENCRYPTION; DATA-SECURITY; CLOUD; SCHEME; PROTOCOL; PRIVACY; STORAGE; ISSUES. Cloud computing is the emergent technology that face one of the significant issues time with data security while outsourcing the data onto the cloud in recent. Some cryptographic techniques have been used for protection in form of identity, attributes and prediction algorithms nonetheless these algorithms lack their performance and becomes are very prone to attackers when an unauthorized user reunited the system with dissimilar way for privileges to the similar data files. The essential need of this data security solved by some enhanced cryptographic techniques in DRM utilizing a secure privacy preserving data sharing with encryption techniques of Dynamic Unidirectional Proxy Re-Encryption. This technique is based on Cipher text Policy Attribute by providing the privacy, integrity and security of the data while retrieving.""Do Electronic Word-of-Mouth and Elaboration Likelihood Model Influence Hotel Booking?; Artificial neural networks; booking intention; demographic; elaboration-likelihood model; eWoM; USER-GENERATED CONTENT; CLOUD-BASED VLE; MODERATING ROLE; BEHAVIORAL INTENTION; MOBILE ENTERTAINMENT; SOURCE CREDIBILITY; NETWORK ANALYSIS; ONLINE REVIEWS; INITIAL TRUST; EWOM. The emergences of Web 2.0 and cloud computing have contributed greatly to the prevalence of electronic Word-of-Mouth (eWoM). Unlike most of the existing studies which have used linear models, nonlinear relationships were discovered in hotel booking intention. So far, the effects of elaboration-likelihood model (ELM) and demographics have been largely overlooked, though studies have shown that ELM may explain consumers' perception, behavior, and IS acceptance. Data were gathered from 497 patrons of 10 hotels in Golden Triangle, Kuala Lumpur, Malaysia. Using artificial neural network (ANN), we found that user involvement, positive eWoM, user expertise, perceived credibility, education, negative eWoM, and income are among the important predictors explaining 81% of variance in booking intention. The theoretical implications may further advance eWoM and ELM literatures, while the managerial implications may provide novel understandings to hotel operators, advertisers, and relevant hospitality policy makers in formulating effective decision making.""Data Lakes, Clouds, and Commons: A Review of Platforms for Analyzing and Sharing Genomic Data; ; CANCER; VISION; BROWSER; GALAXY. Data commons collate data with cloud computing infrastructure and commonly used software services, tools, and applications to create biomedical resources for the large-scale management, analysis, harmonization, and sharing of biomedical data. Over the past few years, data commons have been used to analyze, harmonize, and share large-scale genomics datasets. Data ecosystems can be built by interoperating multiple data commons. It can be quite labor intensive to curate, import, and analyze the data in a data commons. Data lakes provide an alternative to data commons and simply provide access to data, with the data curation and analysis deferred until later and delegated to those that access the data. We review software platforms for managing, analyzing, and sharing genomic data, with an emphasis on data commons, but also cover data ecosystems and data lakes.""The design of an IoT-GIS platform for performing automated analytical tasks; Internet of things; Automated analytical tasks; Mobility context; Smart transit; INTERNET; THINGS; ARCHITECTURE; INTEGRATION. Society has a very ambitious vision of building smart interconnected cities through the Internet of Things (Ion. Billions of data streams will be generated by devices using different networking infrastructures of smart cities, enabling the automation of how the data that are being collected can be analysed for. However, significant scientific and technological challenges need to be overcome before IoT-GIS platforms can be widely used. This paper is a first step towards designing an IoT-GIS platform for performing automated analytical tasks that are able to retrieve, integrate and contextualize data streams with the purpose of adding value to the provision of transit services. Three automated tasks are used to describe our platform: (1) data ingestion for retrieving data streams; (2) data cleaning for handling missing and redundant data streams; and (3) data contextualization for representing the mobility context of transit driving behaviour. The Codiac Transit System of the Greater Moncton area, NB, Canada was used for building a mobility context and evaluating the cloud architecture that was used to implement our IoT-GIS platform. From the experimental results, the need for cloud computing for achieving scalability and high performance of our IoT-GIS platform is validated. Suggestions for the operational management of routes to improve service quality are proposed based on the analytical outcomes.""Toward Ultra-Low-Power Remote Health Monitoring: An Optimal and Adaptive Compressed Sensing Framework for Activity Recognition; Compressed sensing; activity recognition; feature selection; energy efficiency; ultra-low power; optimization; adaptive; WIRELESS SENSOR NETWORKS. Activity recognition, as an important component of behavioral monitoring and intervention, has attracted enormous attention, especially in Mobile Cloud Computing (MCC) and Remote Health Monitoring (RHM) paradigms. While recently resource constrained wearable devices have been gaining popularity, their battery life is limited and constrained by the frequent wireless transmission of data to more computationally powerful back-ends. This paper proposes an ultra-low power activity recognition system using a novel adaptive compressed sensing technique that aims to minimize transmission costs. Coarse-grained on-body sensor localization and unsupervised clustering modules are devised to autonomously reconfigure the compressed sensing module for further power saving. We perform a thorough heuristic optimization using Grammatical Evolution (GE) to ensure minimal computation overhead of the proposed methodology. Our evaluation on a real-world dataset and a low power wearable sensing node demonstrates that our approach can reduce the energy consumption of the wireless data transmission up to 81.2 and 61.5 percent, with up to 60.6 and 35.0 percent overall power savings in comparison with baseline and a naive state-of-the-art approaches, respectively. These solutions lead to an average activity recognition accuracy of 89.0 percent-only 4.8 percent less than the baseline accuracy-while having a negligible energy overhead of on-node computation.""A mixed-method empirical study of Function-as-a-Service software development in industrial practice; Cloud computing; Serverless; Function-as-a-Service; Empirical research; MICROSERVICES. Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of ""serverless"" computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the ""glue"" that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers.""Harnessing the power of big data analytics in the cloud to support learning analytics in mobile learning environment; Mobile learning (m-learning); Learning analytics; Big data analytics; Cloud computing; Map reduce technique; Technology acceptance model (TAM); TECHNOLOGY; ACCEPTANCE. Technology enhanced learning (TEL) such as online learning environment with adaptive technologies has gained growing interest in recent past in the field of teaching and learning. In this context, mobile learning has got much momentum and is exemplified by diverse characteristics associated with the technologies and devices used, the enormous size of data generated throughout a learning session, and the interactions among the learners that occur outside the classroom. Consequently, sophisticated data analysis techniques are required to handle the intricacy of mobile learning and analyze the vast amount of datasets to enhance the learning experiences of mobile learners. This has led to the adoption of big data analytics for efficient processing of big learning data to add value to the mobile learning environments. Yet limited processing capability of the mobile devices is another key challenge faced by such big data analytics in mobile learning environments. To overcome this limitation, certain heavy computational parts could be offloaded to the cloud which can provide enough computation and storage resources. To this end, this paper presents a cloud based mobile learning framework that utilizes big data analytics technique to extract values from huge volume of mobile learners' data. Finally, we investigate learners' readiness and driving factors of mobile learning adoption in higher education institutions. In particular, we propose a hypothesized model for mobile learning adoption built on a locally extended technology acceptance model (TAM).""A dynamic information platform for underground coal mine safety based on internet of things; Internet of things; Big data; Underground coal mine; Mining safety; BIG DATA; IOT; DESIGN; ACCIDENTS; FUTURE; CITIES; SYSTEM. Mechanical and intelligent mining technology reduces labor intensity, accidents and casualties while improving mining efficiency in underground coal mines. Such advanced mining methods are gradually realized in China with increasing application of an Internet of Things (IoT) technique. In this study, a dynamic information platform is established for underground coal mine based on IoT technology. The platform aims to satisfy various objectives of potential users through combining four subsystems, including data acquisition, transmission, analyzation and application systems. To achieve such functions, six functional layers are composed of the platform, including the supporting layer, perception layer, transmission layer, service layer, data extraction layer and application layer. The platform can monitor and record working conditions data of the coal mine production systems as well as location information of underground equipment and miners. Based on cloud computing techniques, big data related to underground coal mining can be analyzed quickly, and critical data associated with user requirements can be extracted precisely. The platform provides a 3D virtual mine system, safety diagnosis system, safety inspection system and emergency rescue system for coal mines. With the established platform, the danger-spotting and subsequent decision-making capabilities of users can be greatly enhanced to ensure underground mining safety.""Efficient 3D probabilistic stability analysis of rock tunnels using a Lattice Model and cloud computing; Lattice; Probabilistic; Rock tunnel; Cloud computing; NUMERICAL-SIMULATION; STOCHASTIC-ANALYSIS; FRACTURE; STRENGTH; BLOCK; NETWORKS; GEOMETRY; BEHAVIOR; PARTICLE. In this paper, the Classic Lattice Spring Model is developed in order to be applied to probabilistic stability analyses of rock tunnels in low in situ stress environments, where the falling of blocks is the main failure mechanism. For this objective, the method is combined with the Synthetic Rock Mass technique and Barton-Bandis joint constitutive model, giving it a series of advantages: simulations with deformable blocks; possible formation of tension cracks; and an appropriate constitutive model for the mechanical behavior of joints. The use of cloud computing technology is proposed for efficient probabilistic analyses using the Monte Carlo simulation. A case study is presented based on an unsupported section of a shallow rock tunnel excavated in the state of Espirito Santo, Brazil. A 3D probabilistic stability analysis of the tunnel is performed with the proposed methodology. From the results, a positional probability map is elaborated, which indicates the likelihood of block failure around the excavation cross-section. The unstable zones indicated by the map are then compared to the failed region of the real tunnel so as to demonstrate the competence of the methodology.""Resource allocation and routing in parallel multi-server queues with abandonments for cloud profit maximization; Parallel multi-server queues; Abandonments; Firm deadlines; Resource allocation; Routing; Markov decision process; Cloud computing; Bernoulli splitting; Index policies; LOAD DISTRIBUTION; OPTIMAL REPAIR; REAL-TIME; CUSTOMERS; OPTIMALITY; ADMISSION; SYSTEMS; SERVICE. This paper considers a Markov decision model for profit maximization of a cloud computing service provider catering to customers submitting jobs with firm real-time random deadlines. Customers are charged on a per-job basis, receiving a full refund if deadlines are missed. The service provider leases computing resources from an infrastructure provider in a two-tier scheme: long-term leasing of basic infrastructure, consisting of heterogeneous parallel service nodes, each modeled as a multi-server queue, and short-term leasing of external servers. Given the intractability of computing an optimal dynamic resource allocation and job routing policy, maximizing the long-run average profit rate, the paper addresses the design, implementation and testing of low-complexity heuristics. The policies considered are a static policy given by an optimal Bernoulli splitting, and three dynamic index policies based on different index definitions: individually optimal (IO), policy improvement (PI) and restless bandit (RB) indices. The paper shows how to implement efficiently each such policy, and presents a comprehensive empirical comparison, drawing qualitative insights on their strengths and weaknesses, and benchmarking their performance in an extensive study.""On semantic detection of cloud API (anti)patterns; Cloud computing; REST; OCCI; Pattern; Anti-pattern; Analysis; Specification; Detection; Ontology; . Context: Open standards are urgently needed for enabling software interoperability in Cloud Computing. Open Cloud Computing Interface (OCCI) provides a set of best design principles to create interoperable REST management APIs. Although OCCI is the only standard addressing the management of any kind of cloud resources, it does not support a range of best principles related to REST design. This often worsens REST API quality by decreasing their understandability and reusability. Objective: We aim at assisting cloud developers to enhance their REST management APIs by providing a compliance evaluation of OCCI and REST best principles and a recommendation support to comply with these principles. Method: First, we leverage patterns and anti-patterns to drive respectively the good and poor practices of OCCI and REST best principles. Then, we propose a semantic-based approach for defining and detecting REST and OCCI (anti)pattems and providing a set of correction recommendations to comply with both REST and OCCI best principles. We validated this approach by applying it on cloud REST APIs and evaluating its accuracy, usefulness and extensibility. Results: We found that our approach accurately detects OCCI and REST(anti)patterns and provides useful recommendations. According to the compliance results, we reveal that there is no widespread adoption of OCCI principles in existing APIs. In contrast, these APIs have reached an acceptable level of maturity regarding REST principles. Conclusion: Our approach provides an effective and extensible technique for defining and detecting OCCI and REST (anti)patterns in Cloud REST APIs. Cloud software developers can benefit from our approach and defined principles to accurately evaluate their APIs from OCCI and REST perspectives. This contributes in designing interoperable, understandable, and reusable Cloud management APIs. Thank to the compliance analysis and the recommendation support, we also contribute to improving these APIs, which make them more straightforward.""Modelling the utilization of cloud health information systems in the Iraqi public healthcare sector; Health information system; Health informatics; e-Health; Cloud computing; Model; Iraq; TECHNOLOGY ACCEPTANCE MODEL; CRITICAL SUCCESS FACTORS; PERCEIVED CONTROL; USER ACCEPTANCE; PRIVACY; DETERMINANTS; EXTENSION; CONTINUANCE; ADOPTION; INTENTIONS. The Iraqi healthcare sector has been suffering from health records management issues from the perspectives of low information technology integrity and data complexity. As a solution, cloud computing services can offer an alternative, low-cost, and reliable way to store, manage, and retrieve health-related data, as well as monitoring patients' health conditions anywhere and anytime using any device with any platform. The migration to cloud services is not yet widespread in Iraqi health facilities due to various challenges, including security and privacy, legal policies, and implementation. For instance, there has been no research shedding light on the utilization of cloud computing services in Iraqi hospitals' health information systems. This study proposed a model by defining the critical success factors influencing physicians' confirmation and behavioral control toward utilizing cloud health information systems in Iraqi hospitals. The model's variables were statistically investigated by utilizing an online questionnaire. Data were collected from a probability sample of 259 physicians working in four Iraqi high-IT hospitals. The collected ordinal data were analyzed using the PLS-SEM approach as a nonparametric second generation multivariate analysis. The results showed that the effects of system compatibility, system complexity, security, and privacy on physicians' confirmation and behavioral control were statistically significant. Both confirmation and behavioral control had a positive effect on physicians' utilization of the technology in the Iraqi hospitals. It is believed that such finding may help to aid the current understanding of cloud health systems in managing health data as well as providing the necessary recommendations for policy makers to direct healthcare professionals to continuously consider the use of modern information and communications technology in the workplace.""Exploring cloud-based Web Processing Service: A case study on the implementation of CMAQ as a Service; CMAQ; WPS; Cloud computing; Earth science model; Web service; OGC; MODELS; ECOSYSTEM; QUALITY; SYSTEM. As an important tool for air quality simulation, the Community Multiscale Air Quality (CMAQ) model is widely used in the environmental modeling community. However, setting up and running the CMAQ model could be challenging for many scientists, especially when they have limited computing resources and little experience in handling large-scale input data. In this study, we explore the cloud-based Web Processing Service (WPS) and present the Cloud WPS framework to support implementing Earth science model as WPS. Specifically, to make CMAQ easier to use for scientists through the latest standard-based Web service technology, CMAQ-WS, a prototype of CMAQ as a Service, is developed and tested. The result of the experiment shows the framework significantly improves not only the performance of but also ease of use of the CMAQ model thus providing great benefits to the environmental modeling community. Meanwhile, the proposed framework provides a general solution to integrate Earth science model, WPS, and cloud infrastructure, which can greatly reduce the workload of Earth scientists.""BMMI-tree: A Peer-to-Peer m-ary tree using 1-m node splitting for an efficient multidimensional complex query search; Distributed computing; Peer-to-peer networks; Multidimensional indexing; Node splitting; Multiway tree structures; Complex query search; DISCOVERY; SERVICE; OVERLAY. Peer-to-Peer (P2P) applications such as content distribution and sharing (like file, audio, video), multiuser communication (games, desktop sharing, e-learning) have emerged as a new paradigm over a last decade. However, scalability requirements remain a major concern and hence, the distribution and effective search of multidimensional data have become major challenges for P2P computing. Most of the existing P2P overlays either do not give support to Multidimensional Indexing (MI) or the frameworks are less efficient for complex query search or they are limited up to binary trees only, with the search complexity O(log(2)N). However, traditional MI based on m-ary tree is strengthened for the complex query search (bound to log(m)N) using higher fanout, m > 2. Based on these observations, we propose BMMI-tree (Balanced Multiway Multidimensional Indexing-tree) that uses an m-ary P2P tree overlay network and also provides the support of MI tree indexing methods such as R-tree or SS-tree in this paper. The paper also analyzes the complex query search algorithms performed in O(log(m)N) steps with the experimental results. In addition, the construction of the P2P tree network requires to split some existing node and its data objects into m new child nodes (during node join) and vice versa (during node leave). To the best of our knowledge, none of the existing node splitting algorithms for multiway multidimensional trees offer 1-m node splitting. Hence, in this paper, we also propose two different approaches to split the MI tree node into m number of nodes (m-ary split) to be used effectively to create a dynamic tree overlay. Lastly, we present how the BMMI-tree can be applied for service provisioning in cloud computing in a decentralized and distributed manner.""QaMeC: A QoS-driven IoVs application optimizing deployment scheme in multimedia edge clouds; IoVs (Internet of Vehicles); IoT(Internet of Things); Optimizing deployment; Cloud computing; Edge computing; Multi-clouds; QoS; CDN (Content Delivery Network, Content Distribution Network); . Deploying applications to a centralized cloud for service delivery is infeasible because of the excessive latency and bandwidth limitation of the Internet, such as transporting all IoVs data to big data processing service in a centralized cloud. Therefore, multi-clouds, especially multiple edge clouds is a rising trend for cloud service provision. However, heterogeneity of the cloud service, complex deployment requirements, and large problem space of multi-clouds deployment make how to deploy applications in the multi-clouds environment be a difficult and error-prone decision-making process. Due to these difficulties, current SIA-based solution lacks a unified model to represent functional and non-functional requirements of users. In this background, we propose a QoS-driven IoVs application optimizing deployment scheme in multimedia edge clouds (QaMeC). Our scheme builds a unified QoS model to shield off the inconsistency of QoS calculation. Moreover, we use NSGA-II algorithm to solve the multi-clouds application deployment problem. The implementation and experiments show that our QaMeC scheme can provide optimal and efficient service deployment solutions for a variety of applications with different QoS requirements in CDN multimedia edge clouds environment.""ITEMa: A methodological approach for cognitive edge computing IoT ecosystems; IoT-based ecosystems; Cognitive systems; Edge and cloud computing; Smart office; Activity recognition; SMART ENVIRONMENTS; INTERNET; SYSTEMS; THINGS; PLATFORM; DESIGN. The ever-increasing spread of Internet of Things (IoT)-based technologies paired with the diffusion of the edge-based computing boosts the development of pervasive cyber ecosystems having the goal of improving the life quality of people and assisting them in daily activities. In this context, cognitive behaviors are purposely required to make such ecosystems able to adapt to people needs and to envisage their behaviors. Despite the growing interest in cognitive ecosystems, still there is a lack of methodological approaches devoted to supporting the design and implementation of such complex systems. This paper proposes ITEMa, an Iot-based smarT Ecosystem Modeling Approach based on a three-layered architecture offering some well-suited abstractions tailored to the development of IoT-based ecosystems which exhibit cognitive behaviors and are able to exploit computational resources located either at the edge of the network or in the Cloud. The effectiveness of the approach is demonstrated through a case study concerning the development of a Smart Office devoted to forecast some usual office activities and to properly adapt the office environmental conditions to them.""Speed up genetic algorithms in the cloud using software containers; Genetic algorithms; Parallel genetic algorithms; Cloud computing; Software containers; Software engineering; . Scalability issues might prevent Genetic Algorithms from being applied to real world problems. Exploiting parallelisation in the cloud might be an affordable approach to getting time efficient solutions that benefit of the appealing features of scalability, resource discovery, reliability, fault-tolerance and costeffectiveness. Nevertheless, parallel computation is very prone to cause considerable overhead for communication. Also, making Genetic Algorithms distributed in an on-demand fashion is not trivial. Aiming at keeping under control the communication cost and, at the same time, supporting developers in the construction and deployment of parallel Genetic Algorithms, we designed and implemented a novel approach to distribute Genetic Algorithms in the form of a cloud-based application. It is based on the master/slave model, exploiting software containers, their cloud orchestration and message queues. We also devised a conceptual workflow covering each cloud Genetic Algorithms distribution phase, from resources allocation to actual deployment and execution, in an engineered fashion. Then, the application performance has been evaluated using a benchmark problem. According to the performance and setup times results, it emerged that the cloud can be considered a compelling way of scaling Genetic Algorithms and an excellent alternative to other technologies strictly related to the physically owned hardware.""Zeus: A resource allocation algorithm for the cloud of sensors; Cloud of sensors; Edge computing; Resource allocation; Mixed integer non-linear programming; Data provisioning; Data freshness; WIRELESS SENSOR; ENERGY-EFFICIENT; INTERNET; PERFORMANCE; SELECTION; SYSTEM. The cloud of sensors (CoS) paradigm brings together cloud computing, edge computing and wireless sensor and actuator networks (WSAN) in the form of a three-tier CoS architecture. By employing CoS virtualization, a set of virtual nodes (VNs) is made available to applications, decoupling them from the CoS infrastructure. Assigning VNs to application requests, in order to timely and efficiently meet application requirements, gives rise to the challenge of resource allocation in CoS. In this work, we formulate the problem of resource allocation in CoS and propose Zeus, a hybrid (partly-decentralized) algorithm for solving it. Zeus has two key features. First, it is able to identify requests that are common for multiple applications and perform only once their required tasks, sharing the results among the applications for saving resources. Second, its hybrid approach leveraging edge computing, makes Zeus scalable in terms of the number of VNs and applications supported, and suitable for meeting delay-sensitive applications.""Cross-layer multiuser session control for optimized communications on SDN-based cloud platforms; Cloud computing; Internet of things; Software defined networking; Cross-layer control plane; VIDEO; QUALITY; DELIVERY; SERVICE; ARCHITECTURE; EXPERIENCE; INTERNET; ISSUES; THINGS. The plethora of emerging applications and their increasing popularity has led to the cloudification of the Internet of Things (IoT). The IoT-cloudification scales up data centers to a high computational resource capacity, to suit the requirements of a large number of users wanting to access multiple services offered by revolutionary IoT applications which, in turn, consume huge amounts of data produced by billions of things. In such systems, IoT-tailored cloud services (e.g., software platforms and middleware) are responsible for running latency-critical IoT applications in different areas (e.g., healthcare, transport systems, safety, Smart Cities, and many others). These applications expect to produce high-density multimedia flows that must be handled appropriately by the underlying communication infrastructure to obtain transport services of the highest standard. In this work, we propose a novel control plane optimization, which leverages the support of the Software-Defined Networking (SDN) substrate so that it can advance beyond today's IP-based communication systems, which reveals weaknesses (e.g. a limited performance and scalability) in delivering multimedia content through per-flow channels. An attempt is made to overcome these limitations by proposing CLASSICO, a Cross-LAyer Sdn Session COntrol approach that exploits the SDN substrate to offload the flow streaming network-computing task from the IoT cloud platform to selected SDN branching nodes, and hence ensures high timeliness and scalability for the IoT-cloud system as a whole. To achieve this. CLASSICO dynamically builds application-layer sessions, which comprise multiple participant members sharing same interested content, and connects them into optimal multiuser tree structures, whereby only selected branching nodes apply SDN replication function for quality-enhanced transport service. We applied CLASSICO to a multimedia use case, and the results show that it outperforms multicast-capable IoT systems in terms of Quality of Service (QoS) and Quality of Experience (QoE) video assessment metrics.""SOGC: Implementing surrogate object garbage collector management for a Mobile Cloud Environment; big data management; garbage collection; mark-compact; memory management; Mobile Cloud Computing; quality in service; semi-space algorithm; . The aim of an ideal distributed Mobile Cloud environment, the surrogate object, is an agent for a particular Mobile Host in the wired or wireless network with specific data structures and methods. The Surrogate object is a software entity that is hosted on mobile support station and acts on behalf of a mobile host during the disconnection operation. The mobile hosts are registered with their own unique identifier in their cloud environment with their mobile support system. If the registered Mobile host is migrated from one cloud to another, the surrogate object handovers the data or resource management to another cloud with a new object with the same entities of old surrogate object, which leads inefficient. The granularity of object migration the old object is useless after the old object switched over into the new object. The collection of unused surrogate object is a big challenge. This problem will be achieved through the garbage collector technique in Mobile Cloud computing. In this research, we propose the Surrogate Object Garbage Collector (SOGC) model with new middleware consists in detecting the unused surrogate object by Mark-Compact Garbage Collection technique to recycle their object resources. SOGC periodically recycles the idle object and automatically reuses it; whenever it is needed to manage cloud environment, it results in quality-of-execution, low latency, and cost optimization in data management.""Tc-PEDCKS: Towards time controlled public key encryption with delegatable conjunctive keyword search for Internet of Things; Internet of Things; Keywords search; Time control; Keywords guessing attacks; PROXY RE-ENCRYPTION; GUESSING ATTACKS; SECURE; DATABASE; SCHEME; SUBSET. With the widespread adoption of Internet of Things (IoT) and cloud computing, more and more individuals and organizations are outsourcing their IoT data in the cloud server. Under these circumstances, how to protect the privacy and security of the outsourced data becomes a key challenge. Public key encryption with keyword search (PEKS) is a promising technology to solve the problem. In this paper, we present a new kind of PEKS named time controlled public key encryption with delegated conjunctive keyword search (tc-PEDCKS) for IoT deployment. A basic tc-PEDCKS is firstly presented which supports conjunctive keyword search and enables a data user to delegate access right over the data to others in a limited time period. To resist against the keyword guessing attacks, a more secure tc-PEDCKS with designated server construction is also presented. The two constructions are proved secure in the random oracle model. The performance evaluation and experimental results demonstrate that our constructions are practical and feasible for real world IoT scenarios.""A holistic review of Network Anomaly Detection Systems: A comprehensive survey; Intrusion Detection System (IDS); Network Anomaly Detection Systems (NADS); Data pre-processing; Decision Engine (DE); INTRUSION DETECTION SYSTEM; BIG DATA ANALYTICS; CLASSIFICATION; ENSEMBLE; FRAMEWORK; TAXONOMY; ATTACKS; DESIGN. Network Anomaly Detection Systems (NADSs) are gaining a more important role in most network defense systems for detecting and preventing potential threats. The paper discusses various aspects of anomaly-based Network Intrusion Detection Systems (NIDSs). The paper explains cyber kill chain models and cyber-attacks that compromise network systems, Moreover, the paper describes various Decision Engine (DE) approaches, including new ensemble learning and deep learning approaches. The paper also provides more details about benchmark datasets for training and validating DE approaches. Most of NADSs' applications, such as Data Centers, Internet of Things (IoT), as well as Fog and Cloud Computing, are also discussed. Finally, we present several experimental explanations which we follow by revealing various promising research directions."